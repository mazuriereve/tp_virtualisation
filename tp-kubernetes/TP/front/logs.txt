
==> Audit <==
|-----------|------------------------------------------------------------------------------------|----------|--------------|---------|----------------------|----------------------|
|  Command  |                                        Args                                        | Profile  |     User     | Version |      Start Time      |       End Time       |
|-----------|------------------------------------------------------------------------------------|----------|--------------|---------|----------------------|----------------------|
| start     |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:18 CEST | 25 Oct 24 10:21 CEST |
| dashboard |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:23 CEST |                      |
| start     | --driver=docker                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:25 CEST | 25 Oct 24 10:26 CEST |
| ssh       |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:33 CEST |                      |
| stop      |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:33 CEST | 25 Oct 24 10:33 CEST |
| start     | --mount-string=C:\Users\FX506\Desktop\cours\virtualisation:/mnt/virtualisation     | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:33 CEST |                      |
|           | --mount                                                                            |          |              |         |                      |                      |
| delete    |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:34 CEST | 25 Oct 24 10:34 CEST |
| start     | --mount-string=C:\Users\FX506\Desktop\cours\virtualisation:/mnt/virtualisation     | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:34 CEST |                      |
|           | --mount                                                                            |          |              |         |                      |                      |
| stop      |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:36 CEST |                      |
| start     | --mount                                                                            | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:36 CEST |                      |
|           | --mount-string=C:/Users/FX506/Desktop/cours/virtualisation:/mnt/virtualisation     |          |              |         |                      |                      |
| start     | --mount                                                                            | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:38 CEST |                      |
|           | --mount-string=C:/Users/FX506/Desktop/cours/virtualisation:/mnt/virtualisation     |          |              |         |                      |                      |
|           | --force                                                                            |          |              |         |                      |                      |
| stop      |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:39 CEST |                      |
| delete    |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:40 CEST | 25 Oct 24 10:40 CEST |
| stop      |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:42 CEST |                      |
| delete    |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:42 CEST | 25 Oct 24 10:42 CEST |
| start     | --mount                                                                            | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:42 CEST |                      |
|           | --mount-string=C:/Users/FX506/Desktop/cours/virtualisation:/mnt/virtualisation     |          |              |         |                      |                      |
| start     | --mount                                                                            | minikube | minikubeuser | v1.34.0 | 25 Oct 24 10:57 CEST |                      |
|           | --mount-string=C:/Users/FX506/Desktop/cours/virtualisation:/mnt/virtualisation     |          |              |         |                      |                      |
| start     | --mount                                                                            | minikube | minikubeuser | v1.34.0 | 25 Oct 24 11:01 CEST |                      |
|           | --mount-string=/mnt/c/Users/FX506/Desktop/cours/virtualisation:/mnt/virtualisation |          |              |         |                      |                      |
| start     |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 11:09 CEST |                      |
| start     | --mount                                                                            | minikube | minikubeuser | v1.34.0 | 25 Oct 24 11:11 CEST |                      |
|           | --mount-string=/mnt/c/Users/FX506/Desktop/cours/virtualisation:/mnt/virtualisation |          |              |         |                      |                      |
| start     | --driver=docker                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 11:14 CEST |                      |
| start     | --driver=docker --force                                                            | minikube | minikubeuser | v1.34.0 | 25 Oct 24 11:15 CEST |                      |
| start     | --driver=docker --force                                                            | minikube | minikubeuser | v1.34.0 | 25 Oct 24 11:17 CEST |                      |
| start     |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 11:21 CEST |                      |
| delete    |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 11:22 CEST | 25 Oct 24 11:22 CEST |
| start     | --driver=docker --force                                                            | minikube | minikubeuser | v1.34.0 | 25 Oct 24 11:23 CEST | 25 Oct 24 11:23 CEST |
| start     |                                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 13:06 CEST | 25 Oct 24 13:07 CEST |
| start     | --driver=docker                                                                    | minikube | minikubeuser | v1.34.0 | 25 Oct 24 13:44 CEST | 25 Oct 24 13:45 CEST |
| start     |                                                                                    | minikube | minikubeuser | v1.34.0 | 08 Nov 24 08:35 CET  |                      |
| start     |                                                                                    | minikube | minikubeuser | v1.34.0 | 08 Nov 24 08:36 CET  | 08 Nov 24 08:36 CET  |
| service   | nginx -n front                                                                     | minikube | minikubeuser | v1.34.0 | 08 Nov 24 09:09 CET  |                      |
| service   | nginx -n front                                                                     | minikube | minikubeuser | v1.34.0 | 08 Nov 24 09:10 CET  |                      |
| service   | nginx -n front                                                                     | minikube | minikubeuser | v1.34.0 | 08 Nov 24 09:13 CET  |                      |
| service   | nginx -n front                                                                     | minikube | minikubeuser | v1.34.0 | 08 Nov 24 09:14 CET  |                      |
| service   | nginx -n front                                                                     | minikube | minikubeuser | v1.34.0 | 08 Nov 24 09:17 CET  |                      |
| service   | nginx -n front                                                                     | minikube | minikubeuser | v1.34.0 | 08 Nov 24 09:20 CET  |                      |
|-----------|------------------------------------------------------------------------------------|----------|--------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2024/11/08 08:36:06
Running on machine: Hoppy
Binary: Built with gc go1.22.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1108 08:36:06.209325     842 out.go:345] Setting OutFile to fd 1 ...
I1108 08:36:06.209759     842 out.go:397] isatty.IsTerminal(1) = true
I1108 08:36:06.209771     842 out.go:358] Setting ErrFile to fd 2...
I1108 08:36:06.209783     842 out.go:397] isatty.IsTerminal(2) = true
I1108 08:36:06.210293     842 root.go:338] Updating PATH: /home/minikubeuser/.minikube/bin
I1108 08:36:06.210970     842 out.go:352] Setting JSON to false
I1108 08:36:06.212561     842 start.go:129] hostinfo: {"hostname":"Hoppy","uptime":37,"bootTime":1731051329,"procs":39,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"5.15.133.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"06097475-3cc1-445e-89d5-1a23ea76bd62"}
I1108 08:36:06.212674     842 start.go:139] virtualization:  guest
I1108 08:36:06.218807     842 out.go:177] üòÑ  minikube v1.34.0 on Ubuntu 24.04 (amd64)
I1108 08:36:06.224378     842 notify.go:220] Checking for updates...
I1108 08:36:06.224966     842 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1108 08:36:06.225136     842 driver.go:394] Setting default libvirt URI to qemu:///system
I1108 08:36:06.302878     842 docker.go:123] docker version: linux-27.2.0:Docker Desktop  ()
I1108 08:36:06.302946     842 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1108 08:36:06.521263     842 info.go:266] docker info: {ID:19764d26-9627-49a2-bd52-8407ca2aa2db Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:74 SystemTime:2024-11-08 07:36:06.512222876 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8245977088 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.2.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8fc6bcff51318944179630522a095cc9dbf9f353 Expected:8fc6bcff51318944179630522a095cc9dbf9f353} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.2-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.2-desktop.2] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.13.0]] Warnings:<nil>}}
I1108 08:36:06.521451     842 docker.go:318] overlay module found
I1108 08:36:06.523579     842 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1108 08:36:06.525246     842 start.go:297] selected driver: docker
I1108 08:36:06.525251     842 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/minikubeuser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1108 08:36:06.525302     842 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1108 08:36:06.525358     842 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1108 08:36:06.738279     842 info.go:266] docker info: {ID:19764d26-9627-49a2-bd52-8407ca2aa2db Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:74 SystemTime:2024-11-08 07:36:06.730491302 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8245977088 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.2.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8fc6bcff51318944179630522a095cc9dbf9f353 Expected:8fc6bcff51318944179630522a095cc9dbf9f353} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.2-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.2-desktop.2] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.13.0]] Warnings:<nil>}}
I1108 08:36:06.738711     842 cni.go:84] Creating CNI manager for ""
I1108 08:36:06.738720     842 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1108 08:36:06.738761     842 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/minikubeuser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1108 08:36:06.740811     842 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1108 08:36:06.742708     842 cache.go:121] Beginning downloading kic base image for docker with docker
I1108 08:36:06.744650     842 out.go:177] üöú  Pulling base image v0.0.45 ...
I1108 08:36:06.746685     842 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1108 08:36:06.746759     842 preload.go:146] Found local preload: /home/minikubeuser/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1108 08:36:06.746765     842 cache.go:56] Caching tarball of preloaded images
I1108 08:36:06.746888     842 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1108 08:36:06.746910     842 preload.go:172] Found /home/minikubeuser/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1108 08:36:06.746916     842 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1108 08:36:06.746970     842 profile.go:143] Saving config to /home/minikubeuser/.minikube/profiles/minikube/config.json ...
W1108 08:36:06.782201     842 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1108 08:36:06.782209     842 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1108 08:36:06.782339     842 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1108 08:36:06.782978     842 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1108 08:36:06.782983     842 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1108 08:36:06.782990     842 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1108 08:36:06.782993     842 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1108 08:36:07.435497     842 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1108 08:36:07.435531     842 cache.go:194] Successfully downloaded all kic artifacts
I1108 08:36:07.435560     842 start.go:360] acquireMachinesLock for minikube: {Name:mkdabd9dc407e5b4abb33014fc03d73a59bda5e2 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1108 08:36:07.435655     842 start.go:364] duration metric: took 77.7¬µs to acquireMachinesLock for "minikube"
I1108 08:36:07.435671     842 start.go:96] Skipping create...Using existing machine configuration
I1108 08:36:07.435682     842 fix.go:54] fixHost starting: 
I1108 08:36:07.436437     842 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1108 08:36:07.457824     842 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1108 08:36:07.457854     842 fix.go:138] unexpected machine state, will restart: <nil>
I1108 08:36:07.459913     842 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1108 08:36:07.461937     842 cli_runner.go:164] Run: docker start minikube
I1108 08:36:07.914437     842 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1108 08:36:07.942081     842 kic.go:430] container "minikube" state is running.
I1108 08:36:07.942456     842 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1108 08:36:07.972448     842 profile.go:143] Saving config to /home/minikubeuser/.minikube/profiles/minikube/config.json ...
I1108 08:36:07.972660     842 machine.go:93] provisionDockerMachine start ...
I1108 08:36:07.972723     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1108 08:36:08.008004     842 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1108 08:36:08.008091     842 ubuntu.go:169] provisioning hostname "minikube"
I1108 08:36:08.008155     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1108 08:36:08.051045     842 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1108 08:36:08.051124     842 machine.go:96] duration metric: took 78.455745ms to provisionDockerMachine
I1108 08:36:08.051216     842 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1108 08:36:08.051252     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:08.087854     842 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57120 SSHKeyPath:/home/minikubeuser/.minikube/machines/minikube/id_rsa Username:docker}
W1108 08:36:08.089616     842 sshutil.go:64] dial failure (will retry): ssh: handshake failed: EOF
I1108 08:36:08.089637     842 retry.go:31] will retry after 181.342308ms: ssh: handshake failed: EOF
W1108 08:36:08.272832     842 sshutil.go:64] dial failure (will retry): ssh: handshake failed: EOF
I1108 08:36:08.272849     842 retry.go:31] will retry after 221.264097ms: ssh: handshake failed: EOF
I1108 08:36:08.604939     842 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1108 08:36:08.608656     842 fix.go:56] duration metric: took 1.172974682s for fixHost
I1108 08:36:08.608670     842 start.go:83] releasing machines lock for "minikube", held for 1.173006682s
W1108 08:36:08.608681     842 start.go:714] error starting host: provision: get ssh host-port: get port 22 for "minikube": docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: exit status 1
stdout:


stderr:
template parsing error: template: :1:4: executing "" at <index (index .NetworkSettings.Ports "22/tcp") 0>: error calling index: reflect: slice index out of range
W1108 08:36:08.608732     842 out.go:270] ü§¶  StartHost failed, but will try again: provision: get ssh host-port: get port 22 for "minikube": docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: exit status 1
stdout:


stderr:
template parsing error: template: :1:4: executing "" at <index (index .NetworkSettings.Ports "22/tcp") 0>: error calling index: reflect: slice index out of range

I1108 08:36:08.608779     842 start.go:729] Will try again in 5 seconds ...
I1108 08:36:13.611407     842 start.go:360] acquireMachinesLock for minikube: {Name:mkdabd9dc407e5b4abb33014fc03d73a59bda5e2 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1108 08:36:13.611629     842 start.go:364] duration metric: took 176.274¬µs to acquireMachinesLock for "minikube"
I1108 08:36:13.611669     842 start.go:96] Skipping create...Using existing machine configuration
I1108 08:36:13.611680     842 fix.go:54] fixHost starting: 
I1108 08:36:13.612275     842 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1108 08:36:13.697065     842 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W1108 08:36:13.697104     842 fix.go:138] unexpected machine state, will restart: <nil>
I1108 08:36:13.709062     842 out.go:177] üèÉ  Updating the running docker "minikube" container ...
I1108 08:36:13.714069     842 machine.go:93] provisionDockerMachine start ...
I1108 08:36:13.714224     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:13.796922     842 main.go:141] libmachine: Using SSH client type: native
I1108 08:36:13.797304     842 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 57120 <nil> <nil>}
I1108 08:36:13.797322     842 main.go:141] libmachine: About to run SSH command:
hostname
I1108 08:36:14.019809     842 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1108 08:36:14.019850     842 ubuntu.go:169] provisioning hostname "minikube"
I1108 08:36:14.020011     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:14.105018     842 main.go:141] libmachine: Using SSH client type: native
I1108 08:36:14.105474     842 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 57120 <nil> <nil>}
I1108 08:36:14.105495     842 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1108 08:36:14.406184     842 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1108 08:36:14.406342     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:14.487474     842 main.go:141] libmachine: Using SSH client type: native
I1108 08:36:14.487970     842 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 57120 <nil> <nil>}
I1108 08:36:14.488004     842 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1108 08:36:14.700940     842 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1108 08:36:14.701008     842 ubuntu.go:175] set auth options {CertDir:/home/minikubeuser/.minikube CaCertPath:/home/minikubeuser/.minikube/certs/ca.pem CaPrivateKeyPath:/home/minikubeuser/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/minikubeuser/.minikube/machines/server.pem ServerKeyPath:/home/minikubeuser/.minikube/machines/server-key.pem ClientKeyPath:/home/minikubeuser/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/minikubeuser/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/minikubeuser/.minikube}
I1108 08:36:14.701134     842 ubuntu.go:177] setting up certificates
I1108 08:36:14.701161     842 provision.go:84] configureAuth start
I1108 08:36:14.701290     842 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1108 08:36:14.787089     842 provision.go:143] copyHostCerts
I1108 08:36:14.794279     842 exec_runner.go:144] found /home/minikubeuser/.minikube/ca.pem, removing ...
I1108 08:36:14.794304     842 exec_runner.go:203] rm: /home/minikubeuser/.minikube/ca.pem
I1108 08:36:14.794463     842 exec_runner.go:151] cp: /home/minikubeuser/.minikube/certs/ca.pem --> /home/minikubeuser/.minikube/ca.pem (1094 bytes)
I1108 08:36:14.795674     842 exec_runner.go:144] found /home/minikubeuser/.minikube/cert.pem, removing ...
I1108 08:36:14.795692     842 exec_runner.go:203] rm: /home/minikubeuser/.minikube/cert.pem
I1108 08:36:14.795786     842 exec_runner.go:151] cp: /home/minikubeuser/.minikube/certs/cert.pem --> /home/minikubeuser/.minikube/cert.pem (1139 bytes)
I1108 08:36:14.797042     842 exec_runner.go:144] found /home/minikubeuser/.minikube/key.pem, removing ...
I1108 08:36:14.797063     842 exec_runner.go:203] rm: /home/minikubeuser/.minikube/key.pem
I1108 08:36:14.797159     842 exec_runner.go:151] cp: /home/minikubeuser/.minikube/certs/key.pem --> /home/minikubeuser/.minikube/key.pem (1675 bytes)
I1108 08:36:14.798172     842 provision.go:117] generating server cert: /home/minikubeuser/.minikube/machines/server.pem ca-key=/home/minikubeuser/.minikube/certs/ca.pem private-key=/home/minikubeuser/.minikube/certs/ca-key.pem org=minikubeuser.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1108 08:36:14.908117     842 provision.go:177] copyRemoteCerts
I1108 08:36:14.908165     842 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1108 08:36:14.908194     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:14.927654     842 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57120 SSHKeyPath:/home/minikubeuser/.minikube/machines/minikube/id_rsa Username:docker}
I1108 08:36:15.020149     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1094 bytes)
I1108 08:36:15.085264     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I1108 08:36:15.143545     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1108 08:36:15.202154     842 provision.go:87] duration metric: took 500.968746ms to configureAuth
I1108 08:36:15.202188     842 ubuntu.go:193] setting minikube options for container-runtime
I1108 08:36:15.202573     842 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1108 08:36:15.202698     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:15.276516     842 main.go:141] libmachine: Using SSH client type: native
I1108 08:36:15.276940     842 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 57120 <nil> <nil>}
I1108 08:36:15.276962     842 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1108 08:36:15.484238     842 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1108 08:36:15.484280     842 ubuntu.go:71] root file system type: overlay
I1108 08:36:15.484577     842 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1108 08:36:15.484737     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:15.565207     842 main.go:141] libmachine: Using SSH client type: native
I1108 08:36:15.565805     842 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 57120 <nil> <nil>}
I1108 08:36:15.565978     842 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1108 08:36:15.834677     842 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1108 08:36:15.834973     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:15.929764     842 main.go:141] libmachine: Using SSH client type: native
I1108 08:36:15.930154     842 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 57120 <nil> <nil>}
I1108 08:36:15.930191     842 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1108 08:36:16.176095     842 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1108 08:36:16.176123     842 machine.go:96] duration metric: took 2.462035422s to provisionDockerMachine
I1108 08:36:16.176140     842 start.go:293] postStartSetup for "minikube" (driver="docker")
I1108 08:36:16.176161     842 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1108 08:36:16.176270     842 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1108 08:36:16.176397     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:16.257914     842 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57120 SSHKeyPath:/home/minikubeuser/.minikube/machines/minikube/id_rsa Username:docker}
I1108 08:36:16.415400     842 ssh_runner.go:195] Run: cat /etc/os-release
I1108 08:36:16.427002     842 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1108 08:36:16.427083     842 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1108 08:36:16.427117     842 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1108 08:36:16.427135     842 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1108 08:36:16.427160     842 filesync.go:126] Scanning /home/minikubeuser/.minikube/addons for local assets ...
I1108 08:36:16.428229     842 filesync.go:126] Scanning /home/minikubeuser/.minikube/files for local assets ...
I1108 08:36:16.429345     842 start.go:296] duration metric: took 253.183634ms for postStartSetup
I1108 08:36:16.429500     842 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1108 08:36:16.429587     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:16.508606     842 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57120 SSHKeyPath:/home/minikubeuser/.minikube/machines/minikube/id_rsa Username:docker}
I1108 08:36:16.653847     842 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1108 08:36:16.669200     842 fix.go:56] duration metric: took 3.057511651s for fixHost
I1108 08:36:16.669227     842 start.go:83] releasing machines lock for "minikube", held for 3.057577916s
I1108 08:36:16.669350     842 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1108 08:36:16.749727     842 ssh_runner.go:195] Run: cat /version.json
I1108 08:36:16.749814     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:16.750869     842 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1108 08:36:16.751028     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:16.842324     842 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57120 SSHKeyPath:/home/minikubeuser/.minikube/machines/minikube/id_rsa Username:docker}
I1108 08:36:16.847647     842 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57120 SSHKeyPath:/home/minikubeuser/.minikube/machines/minikube/id_rsa Username:docker}
I1108 08:36:17.298352     842 ssh_runner.go:195] Run: systemctl --version
I1108 08:36:17.306237     842 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1108 08:36:17.310523     842 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1108 08:36:17.326164     842 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1108 08:36:17.326200     842 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1108 08:36:17.333148     842 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1108 08:36:17.333161     842 start.go:495] detecting cgroup driver to use...
I1108 08:36:17.333185     842 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1108 08:36:17.333290     842 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1108 08:36:17.345093     842 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1108 08:36:17.352647     842 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1108 08:36:17.360077     842 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1108 08:36:17.360209     842 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1108 08:36:17.367717     842 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1108 08:36:17.374930     842 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1108 08:36:17.381984     842 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1108 08:36:17.389437     842 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1108 08:36:17.396207     842 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1108 08:36:17.403375     842 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1108 08:36:17.411093     842 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1108 08:36:17.418412     842 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1108 08:36:17.425674     842 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1108 08:36:17.432037     842 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1108 08:36:17.529812     842 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1108 08:36:17.741175     842 start.go:495] detecting cgroup driver to use...
I1108 08:36:17.741246     842 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1108 08:36:17.741332     842 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1108 08:36:17.773421     842 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1108 08:36:17.773486     842 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1108 08:36:17.792763     842 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1108 08:36:17.825646     842 ssh_runner.go:195] Run: which cri-dockerd
I1108 08:36:17.829332     842 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1108 08:36:17.836492     842 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1108 08:36:17.849969     842 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1108 08:36:17.945282     842 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1108 08:36:18.055323     842 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1108 08:36:18.055433     842 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1108 08:36:18.073381     842 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1108 08:36:18.160868     842 ssh_runner.go:195] Run: sudo systemctl restart docker
I1108 08:36:19.075074     842 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1108 08:36:19.109471     842 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1108 08:36:19.148881     842 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1108 08:36:19.188775     842 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1108 08:36:19.407757     842 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1108 08:36:19.622302     842 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1108 08:36:19.879263     842 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1108 08:36:19.921998     842 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1108 08:36:19.954838     842 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1108 08:36:20.227396     842 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1108 08:36:20.860839     842 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1108 08:36:20.860893     842 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1108 08:36:20.864865     842 start.go:563] Will wait 60s for crictl version
I1108 08:36:20.864903     842 ssh_runner.go:195] Run: which crictl
I1108 08:36:20.868252     842 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1108 08:36:21.119993     842 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1108 08:36:21.120101     842 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1108 08:36:21.437865     842 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1108 08:36:21.519192     842 out.go:235] üê≥  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1108 08:36:21.520335     842 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1108 08:36:21.595040     842 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1108 08:36:21.609106     842 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1108 08:36:21.645706     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1108 08:36:21.725500     842 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/minikubeuser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1108 08:36:21.725761     842 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1108 08:36:21.725900     842 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1108 08:36:21.795185     842 docker.go:685] Got preloaded images: -- stdout --
wordpress:latest
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
mysql:9.0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I1108 08:36:21.795208     842 docker.go:615] Images already preloaded, skipping extraction
I1108 08:36:21.795321     842 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1108 08:36:21.855782     842 docker.go:685] Got preloaded images: -- stdout --
wordpress:latest
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
mysql:9.0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I1108 08:36:21.855808     842 cache_images.go:84] Images are preloaded, skipping loading
I1108 08:36:21.855824     842 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1108 08:36:21.856027     842 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1108 08:36:21.856136     842 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1108 08:36:22.470206     842 cni.go:84] Creating CNI manager for ""
I1108 08:36:22.470239     842 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1108 08:36:22.470265     842 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1108 08:36:22.470307     842 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1108 08:36:22.470550     842 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1108 08:36:22.470656     842 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1108 08:36:22.501868     842 binaries.go:44] Found k8s binaries, skipping transfer
I1108 08:36:22.501980     842 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1108 08:36:22.529089     842 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1108 08:36:22.581840     842 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1108 08:36:22.634662     842 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I1108 08:36:22.693588     842 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1108 08:36:22.705822     842 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1108 08:36:22.740730     842 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1108 08:36:23.002212     842 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1108 08:36:23.046265     842 certs.go:68] Setting up /home/minikubeuser/.minikube/profiles/minikube for IP: 192.168.49.2
I1108 08:36:23.047938     842 certs.go:194] generating shared ca certs ...
I1108 08:36:23.047986     842 certs.go:226] acquiring lock for ca certs: {Name:mk1865d691fb3fb7a592d75bc62f8879a05ea2ac Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1108 08:36:23.049398     842 certs.go:235] skipping valid "minikubeCA" ca cert: /home/minikubeuser/.minikube/ca.key
I1108 08:36:23.050576     842 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/minikubeuser/.minikube/proxy-client-ca.key
I1108 08:36:23.050602     842 certs.go:256] generating profile certs ...
I1108 08:36:23.051660     842 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/minikubeuser/.minikube/profiles/minikube/client.key
I1108 08:36:23.053314     842 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/minikubeuser/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1108 08:36:23.054484     842 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/minikubeuser/.minikube/profiles/minikube/proxy-client.key
I1108 08:36:23.054740     842 certs.go:484] found cert: /home/minikubeuser/.minikube/certs/ca-key.pem (1679 bytes)
I1108 08:36:23.054802     842 certs.go:484] found cert: /home/minikubeuser/.minikube/certs/ca.pem (1094 bytes)
I1108 08:36:23.054846     842 certs.go:484] found cert: /home/minikubeuser/.minikube/certs/cert.pem (1139 bytes)
I1108 08:36:23.054886     842 certs.go:484] found cert: /home/minikubeuser/.minikube/certs/key.pem (1675 bytes)
I1108 08:36:23.057825     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1108 08:36:23.132640     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1108 08:36:23.212574     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1108 08:36:23.291227     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1108 08:36:23.372615     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1108 08:36:23.456049     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1108 08:36:23.539287     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1108 08:36:23.567992     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1108 08:36:23.595232     842 ssh_runner.go:362] scp /home/minikubeuser/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1108 08:36:23.624009     842 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1108 08:36:23.648550     842 ssh_runner.go:195] Run: openssl version
I1108 08:36:23.659884     842 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1108 08:36:23.672888     842 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1108 08:36:23.677156     842 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct 25 08:20 /usr/share/ca-certificates/minikubeCA.pem
I1108 08:36:23.677205     842 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1108 08:36:23.685217     842 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1108 08:36:23.693845     842 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1108 08:36:23.697410     842 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1108 08:36:23.703848     842 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1108 08:36:23.709927     842 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1108 08:36:23.717235     842 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1108 08:36:23.724358     842 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1108 08:36:23.731318     842 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1108 08:36:23.740487     842 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/minikubeuser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1108 08:36:23.740663     842 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1108 08:36:23.830350     842 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1108 08:36:23.851664     842 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1108 08:36:23.851681     842 kubeadm.go:593] restartPrimaryControlPlane start ...
I1108 08:36:23.851882     842 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1108 08:36:23.923302     842 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1108 08:36:23.923402     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1108 08:36:23.977886     842 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:49788"
I1108 08:36:23.977908     842 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:49788, want: 127.0.0.1:57124
I1108 08:36:23.978352     842 kubeconfig.go:62] /home/minikubeuser/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I1108 08:36:23.978824     842 lock.go:35] WriteFile acquiring /home/minikubeuser/.kube/config: {Name:mk8beb05f4dd87c6286465989d7d50dec59bc27e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1108 08:36:24.008136     842 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1108 08:36:24.037005     842 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1108 08:36:24.037041     842 kubeadm.go:597] duration metric: took 185.351067ms to restartPrimaryControlPlane
I1108 08:36:24.037055     842 kubeadm.go:394] duration metric: took 296.580041ms to StartCluster
I1108 08:36:24.037080     842 settings.go:142] acquiring lock: {Name:mk0cd5beeff27af0dad923a9e65788b9bd724adb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1108 08:36:24.037281     842 settings.go:150] Updating kubeconfig:  /home/minikubeuser/.kube/config
I1108 08:36:24.039043     842 lock.go:35] WriteFile acquiring /home/minikubeuser/.kube/config: {Name:mk8beb05f4dd87c6286465989d7d50dec59bc27e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1108 08:36:24.039628     842 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1108 08:36:24.039846     842 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1108 08:36:24.039957     842 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1108 08:36:24.040020     842 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W1108 08:36:24.040038     842 addons.go:243] addon storage-provisioner should already be in state true
I1108 08:36:24.040087     842 host.go:66] Checking if "minikube" exists ...
I1108 08:36:24.040115     842 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1108 08:36:24.040132     842 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1108 08:36:24.040197     842 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1108 08:36:24.041010     842 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1108 08:36:24.041048     842 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1108 08:36:24.047327     842 out.go:177] üîé  Verifying Kubernetes components...
I1108 08:36:24.050685     842 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1108 08:36:24.098971     842 addons.go:234] Setting addon default-storageclass=true in "minikube"
W1108 08:36:24.098981     842 addons.go:243] addon default-storageclass should already be in state true
I1108 08:36:24.099005     842 host.go:66] Checking if "minikube" exists ...
I1108 08:36:24.099402     842 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1108 08:36:24.104961     842 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1108 08:36:24.107079     842 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1108 08:36:24.107089     842 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1108 08:36:24.107149     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:24.130377     842 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1108 08:36:24.130392     842 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1108 08:36:24.130490     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1108 08:36:24.140454     842 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57120 SSHKeyPath:/home/minikubeuser/.minikube/machines/minikube/id_rsa Username:docker}
I1108 08:36:24.167641     842 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57120 SSHKeyPath:/home/minikubeuser/.minikube/machines/minikube/id_rsa Username:docker}
I1108 08:36:24.346722     842 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1108 08:36:24.356736     842 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1108 08:36:24.437678     842 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W1108 08:36:25.325702     842 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1108 08:36:25.325941     842 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1108 08:36:25.325949     842 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1108 08:36:25.325973     842 retry.go:31] will retry after 141.838452ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1108 08:36:25.325907     842 retry.go:31] will retry after 172.408486ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1108 08:36:25.425449     842 api_server.go:52] waiting for apiserver process to appear ...
I1108 08:36:25.425596     842 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1108 08:36:25.468475     842 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1108 08:36:25.498874     842 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1108 08:36:25.925817     842 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1108 08:36:26.423040     842 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1108 08:36:26.423086     842 retry.go:31] will retry after 344.493413ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1108 08:36:26.605511     842 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.106601197s)
W1108 08:36:26.605542     842 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1108 08:36:26.605563     842 retry.go:31] will retry after 327.980195ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1108 08:36:26.605627     842 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1108 08:36:26.768148     842 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1108 08:36:26.926687     842 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1108 08:36:26.934793     842 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1108 08:36:27.220840     842 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1108 08:36:27.220867     842 retry.go:31] will retry after 601.963789ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1108 08:36:27.220970     842 api_server.go:72] duration metric: took 3.194172973s to wait for apiserver process to appear ...
I1108 08:36:27.220980     842 api_server.go:88] waiting for apiserver healthz status ...
I1108 08:36:27.220998     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:27.222668     842 api_server.go:269] stopped: https://127.0.0.1:57124/healthz: Get "https://127.0.0.1:57124/healthz": EOF
W1108 08:36:27.240708     842 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1108 08:36:27.240747     842 retry.go:31] will retry after 778.026812ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1108 08:36:27.721862     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:27.730393     842 api_server.go:269] stopped: https://127.0.0.1:57124/healthz: Get "https://127.0.0.1:57124/healthz": EOF
I1108 08:36:27.824163     842 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1108 08:36:28.019742     842 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1108 08:36:28.221602     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:28.222447     842 api_server.go:269] stopped: https://127.0.0.1:57124/healthz: Get "https://127.0.0.1:57124/healthz": EOF
I1108 08:36:28.721277     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:33.509245     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1108 08:36:33.509287     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1108 08:36:33.509320     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:33.607485     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1108 08:36:33.607530     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1108 08:36:33.721995     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:33.820231     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W1108 08:36:33.820281     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I1108 08:36:34.221946     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:34.239352     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1108 08:36:34.239385     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1108 08:36:34.454652     842 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (6.630424747s)
I1108 08:36:34.722200     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:34.739994     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1108 08:36:34.740040     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1108 08:36:35.221898     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:35.316137     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1108 08:36:35.316196     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1108 08:36:35.722721     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:35.825037     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1108 08:36:35.825092     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1108 08:36:36.221521     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:36.312502     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1108 08:36:36.312552     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1108 08:36:36.721644     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:36.814781     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1108 08:36:36.814831     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1108 08:36:37.222151     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:37.308157     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1108 08:36:37.308217     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1108 08:36:37.721271     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:37.812182     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1108 08:36:37.812231     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1108 08:36:38.221812     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:38.305530     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1108 08:36:38.305558     842 api_server.go:103] status: https://127.0.0.1:57124/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1108 08:36:38.721135     842 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57124/healthz ...
I1108 08:36:38.814939     842 api_server.go:279] https://127.0.0.1:57124/healthz returned 200:
ok
I1108 08:36:38.820089     842 api_server.go:141] control plane version: v1.31.0
I1108 08:36:38.820142     842 api_server.go:131] duration metric: took 11.599145975s to wait for apiserver health ...
I1108 08:36:38.820302     842 system_pods.go:43] waiting for kube-system pods to appear ...
I1108 08:36:38.940472     842 system_pods.go:59] 7 kube-system pods found
I1108 08:36:38.940516     842 system_pods.go:61] "coredns-6f6b679f8f-g7p7x" [621445d7-92e5-4de3-9542-552e098c287b] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1108 08:36:38.940534     842 system_pods.go:61] "etcd-minikube" [7c44b1b8-054a-455f-8d70-a7203864ff32] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1108 08:36:38.940549     842 system_pods.go:61] "kube-apiserver-minikube" [9e91b34e-01b8-4bd1-bfcd-54fb9e58d7f5] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1108 08:36:38.940562     842 system_pods.go:61] "kube-controller-manager-minikube" [fcf9c29e-0b64-4870-b8df-6c28c5996d27] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1108 08:36:38.940575     842 system_pods.go:61] "kube-proxy-zx86f" [a8500fda-6965-4791-a88c-75b1b6650fc1] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1108 08:36:38.940587     842 system_pods.go:61] "kube-scheduler-minikube" [5d4bc488-c9a7-44ff-ae6e-02eef1247667] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1108 08:36:38.940598     842 system_pods.go:61] "storage-provisioner" [3175dbed-8aee-4b67-93dd-a06cf95ae9fe] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1108 08:36:38.940612     842 system_pods.go:74] duration metric: took 120.240898ms to wait for pod list to return data ...
I1108 08:36:38.940634     842 kubeadm.go:582] duration metric: took 14.913831173s to wait for: map[apiserver:true system_pods:true]
I1108 08:36:38.940658     842 node_conditions.go:102] verifying NodePressure condition ...
I1108 08:36:39.229083     842 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1108 08:36:39.229143     842 node_conditions.go:123] node cpu capacity is 8
I1108 08:36:39.229194     842 node_conditions.go:105] duration metric: took 288.526849ms to run NodePressure ...
I1108 08:36:39.229232     842 start.go:241] waiting for startup goroutines ...
I1108 08:36:43.315351     842 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (15.295540863s)
I1108 08:36:43.322072     842 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I1108 08:36:43.334720     842 addons.go:510] duration metric: took 19.307739902s for enable addons: enabled=[default-storageclass storage-provisioner]
I1108 08:36:43.334883     842 start.go:246] waiting for cluster config update ...
I1108 08:36:43.334916     842 start.go:255] writing updated cluster config ...
I1108 08:36:43.335513     842 ssh_runner.go:195] Run: rm -f paused
I1108 08:36:43.977538     842 start.go:600] kubectl: 1.30.2, cluster: 1.31.0 (minor skew: 1)
I1108 08:36:43.979566     842 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 08 07:36:18 minikube dockerd[1159]: time="2024-11-08T07:36:18.982428227Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Nov 08 07:36:18 minikube dockerd[1159]: time="2024-11-08T07:36:18.982461287Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Nov 08 07:36:18 minikube dockerd[1159]: time="2024-11-08T07:36:18.982476415Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Nov 08 07:36:18 minikube dockerd[1159]: time="2024-11-08T07:36:18.982526205Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Nov 08 07:36:18 minikube dockerd[1159]: time="2024-11-08T07:36:18.982742899Z" level=info msg="Daemon has completed initialization"
Nov 08 07:36:19 minikube dockerd[1159]: time="2024-11-08T07:36:19.069446072Z" level=info msg="API listen on /var/run/docker.sock"
Nov 08 07:36:19 minikube dockerd[1159]: time="2024-11-08T07:36:19.069670180Z" level=info msg="API listen on [::]:2376"
Nov 08 07:36:19 minikube systemd[1]: Started Docker Application Container Engine.
Nov 08 07:36:20 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 08 07:36:20 minikube cri-dockerd[1468]: time="2024-11-08T07:36:20Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Nov 08 07:36:20 minikube cri-dockerd[1468]: time="2024-11-08T07:36:20Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 08 07:36:20 minikube cri-dockerd[1468]: time="2024-11-08T07:36:20Z" level=info msg="Start docker client with request timeout 0s"
Nov 08 07:36:20 minikube cri-dockerd[1468]: time="2024-11-08T07:36:20Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 08 07:36:20 minikube cri-dockerd[1468]: time="2024-11-08T07:36:20Z" level=info msg="Loaded network plugin cni"
Nov 08 07:36:20 minikube cri-dockerd[1468]: time="2024-11-08T07:36:20Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 08 07:36:20 minikube cri-dockerd[1468]: time="2024-11-08T07:36:20Z" level=info msg="Setting cgroupDriver cgroupfs"
Nov 08 07:36:20 minikube cri-dockerd[1468]: time="2024-11-08T07:36:20Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 08 07:36:20 minikube cri-dockerd[1468]: time="2024-11-08T07:36:20Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 08 07:36:20 minikube cri-dockerd[1468]: time="2024-11-08T07:36:20Z" level=info msg="Start cri-dockerd grpc backend"
Nov 08 07:36:20 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 08 07:36:23 minikube cri-dockerd[1468]: time="2024-11-08T07:36:23Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-6884d88685-cgh88_front\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"60f86719857ae2ecbc802f9059460118a1871eaa73027ec66cd28c29a8a406e2\""
Nov 08 07:36:23 minikube cri-dockerd[1468]: time="2024-11-08T07:36:23Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-6884d88685-cgh88_front\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a1a84147df9b548434610eceda02d3ebd61370d08df3bdee23dd143aefbc1118\""
Nov 08 07:36:23 minikube cri-dockerd[1468]: time="2024-11-08T07:36:23Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mysql-0_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e7ec99bf27ef960ba177749ff285149fbd5b7e46398039f53255cf3ee2952e57\""
Nov 08 07:36:23 minikube cri-dockerd[1468]: time="2024-11-08T07:36:23Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-6884d88685-kv7qb_front\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"115617ed757ae8a9cd385f9d3b2ae611fda6c9b3771777869d775d21d9e0239a\""
Nov 08 07:36:23 minikube cri-dockerd[1468]: time="2024-11-08T07:36:23Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-6884d88685-kv7qb_front\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8342e261d4cdc8b97b0f849b8f168603c48440852091a7e7c3087e55a0862d89\""
Nov 08 07:36:23 minikube cri-dockerd[1468]: time="2024-11-08T07:36:23Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-6884d88685-zmgnz_front\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"305fdb42c78ef85a73cf72e8fdd5b201fd73ce0424710e266490d566cd5b37e0\""
Nov 08 07:36:24 minikube cri-dockerd[1468]: time="2024-11-08T07:36:23Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-6884d88685-zmgnz_front\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c29cd22295824e203f8dc26221e9d1e9dc092c330f6d5c6e8ca7997354cd9d09\""
Nov 08 07:36:24 minikube cri-dockerd[1468]: time="2024-11-08T07:36:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"wordpress-6ddd454bcb-zmhlk_middle\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"726495bf33da05088bf5afc4821b19a2e2fa97585bfe86b0f809bd6bd2ac7911\""
Nov 08 07:36:24 minikube cri-dockerd[1468]: time="2024-11-08T07:36:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mysql-0_database\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"62a881d013a9873abfb6293f17c9112a4f1e94d493b0319284def268eb34113f\""
Nov 08 07:36:24 minikube cri-dockerd[1468]: time="2024-11-08T07:36:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-g7p7x_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6d4495a5c7b4872148600b9393ed033017f426ba896858defc5da97e81cb94c8\""
Nov 08 07:36:24 minikube cri-dockerd[1468]: time="2024-11-08T07:36:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-g7p7x_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2343798fc643474c3386c0f8bb3fdc42befbde9b57addc488972fa7e2fb583ea\""
Nov 08 07:36:24 minikube cri-dockerd[1468]: time="2024-11-08T07:36:24Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"bd534897e2c75425aaadbb4002d056cec7c96b81afee44c857e7fa79754728d7\". Proceed without further sandbox information."
Nov 08 07:36:25 minikube cri-dockerd[1468]: time="2024-11-08T07:36:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/204bb2538d45289c86da1c9834ecf17ec9854e4237514e65d4c3c85c82b24718/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 08 07:36:25 minikube cri-dockerd[1468]: time="2024-11-08T07:36:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/772e3e658b8d624b46c5ba62680fe1a233fa610f762f51a1483e6301bb6613e2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 08 07:36:25 minikube cri-dockerd[1468]: time="2024-11-08T07:36:25Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-6884d88685-zmgnz_front\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"305fdb42c78ef85a73cf72e8fdd5b201fd73ce0424710e266490d566cd5b37e0\""
Nov 08 07:36:25 minikube cri-dockerd[1468]: time="2024-11-08T07:36:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7c8257d20d42ef3aca4206848e51f070fdadf5fbe92fc945d20b75c2bb428610/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 08 07:36:25 minikube cri-dockerd[1468]: time="2024-11-08T07:36:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1f21375b2193937efadd0bf2c0cac0a836e5cd07d863035f493564f21c8d8130/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 08 07:36:25 minikube cri-dockerd[1468]: time="2024-11-08T07:36:25Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-6884d88685-kv7qb_front\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"115617ed757ae8a9cd385f9d3b2ae611fda6c9b3771777869d775d21d9e0239a\""
Nov 08 07:36:26 minikube cri-dockerd[1468]: time="2024-11-08T07:36:26Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-g7p7x_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6d4495a5c7b4872148600b9393ed033017f426ba896858defc5da97e81cb94c8\""
Nov 08 07:36:26 minikube cri-dockerd[1468]: time="2024-11-08T07:36:26Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-6884d88685-cgh88_front\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"60f86719857ae2ecbc802f9059460118a1871eaa73027ec66cd28c29a8a406e2\""
Nov 08 07:36:34 minikube cri-dockerd[1468]: time="2024-11-08T07:36:34Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Nov 08 07:36:38 minikube cri-dockerd[1468]: time="2024-11-08T07:36:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aada9264a4c0556a4a0a9ed5f22f8b23abea329b7f5457c1281505f2eaac9ff3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 08 07:36:38 minikube cri-dockerd[1468]: time="2024-11-08T07:36:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7ebf3ffbc6d37d1b71315a07d265125702f555108f982b6280d3a05760ff277d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 08 07:36:41 minikube cri-dockerd[1468]: time="2024-11-08T07:36:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/38f6fde38b5a186e5d0e59e24ffae534cf4c07b23729dfa2124b1b3fabb4d3c3/resolv.conf as [nameserver 10.96.0.10 search front.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 08 07:36:41 minikube cri-dockerd[1468]: time="2024-11-08T07:36:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/51548ce12862ff04632173ded29c00eb54f99533c9f8b62b48874643bc2605b1/resolv.conf as [nameserver 10.96.0.10 search front.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 08 07:36:42 minikube cri-dockerd[1468]: time="2024-11-08T07:36:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e541f0caee337df0450c1a5a78d8d9c4d80ce068e926871617b075c362702eec/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 08 07:36:42 minikube cri-dockerd[1468]: time="2024-11-08T07:36:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3bd04b2e92d2eff2364e53c4b631bdeee15312fc1c2a3909b96c44c9b0f86b87/resolv.conf as [nameserver 10.96.0.10 search middle.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 08 07:36:42 minikube cri-dockerd[1468]: time="2024-11-08T07:36:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/40a1a36470704ad36b46ae504855fbfa73a9204b88e996272c17fa80f1025d82/resolv.conf as [nameserver 10.96.0.10 search front.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 08 07:36:43 minikube cri-dockerd[1468]: time="2024-11-08T07:36:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c9de22d35bb010cffa8f9d268cdf11256dbadb25c79a1f4c26d4de293885721e/resolv.conf as [nameserver 10.96.0.10 search database.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 08 07:36:43 minikube cri-dockerd[1468]: time="2024-11-08T07:36:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/28bdc4078e84174ed3e9f867e280fb26aae0043fc6fb163ee4dea20b53acb98f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 08 07:36:56 minikube cri-dockerd[1468]: time="2024-11-08T07:36:56Z" level=info msg="Pulling image wordpress:latest: 3cdb90068a52: Extracting [=========================>                         ]  52.92MB/104.3MB"
Nov 08 07:37:03 minikube cri-dockerd[1468]: time="2024-11-08T07:37:03Z" level=info msg="Stop pulling image wordpress:latest: Status: Downloaded newer image for wordpress:latest"
Nov 08 07:37:05 minikube dockerd[1159]: time="2024-11-08T07:37:05.243499031Z" level=info msg="ignoring event" container=8d5bc5f76cf1806a830e3d89fcad159f83ed4807242338191b7acadc5af5ff05 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 08 08:20:13 minikube cri-dockerd[1468]: time="2024-11-08T08:20:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7e9e043bece60d8e5e5cb9548504f261de2ffb483257debc636e4e7293161c15/resolv.conf as [nameserver 10.96.0.10 search front.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 08 08:20:20 minikube cri-dockerd[1468]: time="2024-11-08T08:20:20Z" level=info msg="Stop pulling image nginx:latest: Status: Downloaded newer image for nginx:latest"
Nov 08 08:20:20 minikube dockerd[1159]: time="2024-11-08T08:20:20.353446973Z" level=info msg="ignoring event" container=738121fcb0ac50945c6cd2df0154badf7f8a2347c91009aaa35647004f7126f7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 08 08:20:22 minikube cri-dockerd[1468]: time="2024-11-08T08:20:22Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Nov 08 08:20:22 minikube dockerd[1159]: time="2024-11-08T08:20:22.401571262Z" level=info msg="ignoring event" container=06c8691532f273b185a26b85e3fd6ebc40b1f445e6de840bae5440545c76ba66 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 08 08:20:37 minikube cri-dockerd[1468]: time="2024-11-08T08:20:37Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Nov 08 08:20:38 minikube dockerd[1159]: time="2024-11-08T08:20:38.036240523Z" level=info msg="ignoring event" container=2075ce0ead2737c92be07c75a90f048fce84642330eb20743f56d2bc6dda1e39 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
2075ce0ead273       nginx@sha256:28402db69fec7c17e179ea87882667f1e054391138f77ffaf0c3eb388efc3ffb       13 seconds ago      Exited              nginx                     2                   7e9e043bece60       nginx-dfddbd46c-4rdpl
7db23772e7efe       6e38f40d628db                                                                       43 minutes ago      Running             storage-provisioner       7                   7ebf3ffbc6d37       storage-provisioner
bac1380403f84       wordpress@sha256:59c479ba37a3ff49d665239fae7b71b890a52e7f55c643b6cde823beba5d598b   43 minutes ago      Running             wordpress                 1                   3bd04b2e92d2e       wordpress-6ddd454bcb-zmhlk
58c99bda83ec4       c757d623b1901                                                                       44 minutes ago      Running             mysql                     1                   28bdc4078e841       mysql-0
ad72323f256b4       295c7be079025                                                                       44 minutes ago      Running             nginx                     2                   40a1a36470704       nginx-deployment-6884d88685-cgh88
222109d2d70d2       cbb01a7bd410d                                                                       44 minutes ago      Running             coredns                   3                   e541f0caee337       coredns-6f6b679f8f-g7p7x
beb90287080e3       295c7be079025                                                                       44 minutes ago      Running             nginx                     2                   38f6fde38b5a1       nginx-deployment-6884d88685-zmgnz
4e503032d51cd       295c7be079025                                                                       44 minutes ago      Running             nginx                     2                   51548ce12862f       nginx-deployment-6884d88685-kv7qb
8d5bc5f76cf18       6e38f40d628db                                                                       44 minutes ago      Exited              storage-provisioner       6                   7ebf3ffbc6d37       storage-provisioner
e81197fb7cd9f       ad83b2ca7b09e                                                                       44 minutes ago      Running             kube-proxy                3                   aada9264a4c05       kube-proxy-zx86f
095666a4e48d7       2e96e5913fc06                                                                       44 minutes ago      Running             etcd                      3                   1f21375b21939       etcd-minikube
406b867db49f0       1766f54c897f0                                                                       44 minutes ago      Running             kube-scheduler            3                   772e3e658b8d6       kube-scheduler-minikube
1b61b93ae3309       045733566833c                                                                       44 minutes ago      Running             kube-controller-manager   3                   204bb2538d452       kube-controller-manager-minikube
28c92e88ff37b       604f5db92eaa8                                                                       44 minutes ago      Running             kube-apiserver            3                   7c8257d20d42e       kube-apiserver-minikube
96c5b21337383       wordpress@sha256:362bc4b4df5a46f5a2bfc51ea53f85f51c0a422860c25ef3e7a8a6622dd7a33c   13 days ago         Exited              wordpress                 0                   726495bf33da0       wordpress-6ddd454bcb-zmhlk
d9324e92b35cf       c757d623b1901                                                                       13 days ago         Exited              mysql                     0                   e7ec99bf27ef9       mysql-0
405d9d4ad0d36       295c7be079025                                                                       13 days ago         Exited              nginx                     1                   305fdb42c78ef       nginx-deployment-6884d88685-zmgnz
86ceb2801013a       cbb01a7bd410d                                                                       13 days ago         Exited              coredns                   2                   6d4495a5c7b48       coredns-6f6b679f8f-g7p7x
abfb7276ffa2a       295c7be079025                                                                       13 days ago         Exited              nginx                     1                   115617ed757ae       nginx-deployment-6884d88685-kv7qb
dd0e293f2ab9b       295c7be079025                                                                       13 days ago         Exited              nginx                     1                   60f86719857ae       nginx-deployment-6884d88685-cgh88
86e33c9aa8f20       604f5db92eaa8                                                                       13 days ago         Exited              kube-apiserver            2                   f90983e98e507       kube-apiserver-minikube
8e20331c4452c       2e96e5913fc06                                                                       13 days ago         Exited              etcd                      2                   5ffb9816d8d95       etcd-minikube
b336c460ee4c3       045733566833c                                                                       13 days ago         Exited              kube-controller-manager   2                   610a430e75f11       kube-controller-manager-minikube
c749d1f9e09d8       1766f54c897f0                                                                       13 days ago         Exited              kube-scheduler            2                   d785ae97f6cee       kube-scheduler-minikube
71fe33837191f       ad83b2ca7b09e                                                                       13 days ago         Exited              kube-proxy                2                   7a2fffb2507bb       kube-proxy-zx86f


==> coredns [222109d2d70d] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:34617 - 17424 "HINFO IN 6346978243782628272.3241434797145949077. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.069355294s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1982248174]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (08-Nov-2024 07:36:45.150) (total time: 21076ms):
Trace[1982248174]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21074ms (07:37:06.206)
Trace[1982248174]: [21.07650008s] [21.07650008s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[2124701157]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (08-Nov-2024 07:36:45.150) (total time: 21076ms):
Trace[2124701157]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21074ms (07:37:06.206)
Trace[2124701157]: [21.076506176s] [21.076506176s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1877452894]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (08-Nov-2024 07:36:45.150) (total time: 21077ms):
Trace[1877452894]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21073ms (07:37:06.206)
Trace[1877452894]: [21.077103718s] [21.077103718s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] 10.244.0.22:55577 - 50591 "AAAA IN wordpress.middle.svc.cluster.local.front.svc.cluster.local. udp 76 false 512" NXDOMAIN qr,aa,rd 169 0.006757939s
[INFO] 10.244.0.22:55577 - 61345 "A IN wordpress.middle.svc.cluster.local.front.svc.cluster.local. udp 76 false 512" NXDOMAIN qr,aa,rd 169 0.006780241s
[INFO] 10.244.0.22:56964 - 15384 "AAAA IN wordpress.middle.svc.cluster.local.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000080308s
[INFO] 10.244.0.22:50090 - 53355 "AAAA IN wordpress.middle.svc.cluster.local.svc.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000126212s
[INFO] 10.244.0.22:50090 - 5737 "A IN wordpress.middle.svc.cluster.local.svc.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000249123s
[INFO] 10.244.0.22:56964 - 52762 "A IN wordpress.middle.svc.cluster.local.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000118711s
[INFO] 10.244.0.22:55114 - 2000 "A IN wordpress.middle.svc.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000165916s
[INFO] 10.244.0.22:55114 - 62930 "AAAA IN wordpress.middle.svc.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000267825s
[INFO] 10.244.0.22:38775 - 19622 "A IN wordpress.middle.svc.cluster.local.front.svc.cluster.local. udp 76 false 512" NXDOMAIN qr,aa,rd 169 0.000179717s
[INFO] 10.244.0.22:38775 - 35749 "AAAA IN wordpress.middle.svc.cluster.local.front.svc.cluster.local. udp 76 false 512" NXDOMAIN qr,aa,rd 169 0.000224822s
[INFO] 10.244.0.22:55141 - 31187 "AAAA IN wordpress.middle.svc.cluster.local.svc.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000049705s
[INFO] 10.244.0.22:55141 - 43213 "A IN wordpress.middle.svc.cluster.local.svc.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000071106s
[INFO] 10.244.0.22:48972 - 14370 "AAAA IN wordpress.middle.svc.cluster.local.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000084108s
[INFO] 10.244.0.22:48972 - 3616 "A IN wordpress.middle.svc.cluster.local.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.00011061s
[INFO] 10.244.0.22:56048 - 36955 "AAAA IN wordpress.middle.svc.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000042404s
[INFO] 10.244.0.22:56048 - 57945 "A IN wordpress.middle.svc.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000037104s
[INFO] 10.244.0.22:35701 - 51530 "AAAA IN wordpress.middle.svc.cluster.local.front.svc.cluster.local. udp 76 false 512" NXDOMAIN qr,aa,rd 169 0.000590852s
[INFO] 10.244.0.22:35701 - 41537 "A IN wordpress.middle.svc.cluster.local.front.svc.cluster.local. udp 76 false 512" NXDOMAIN qr,aa,rd 169 0.000950883s
[INFO] 10.244.0.22:50432 - 57372 "AAAA IN wordpress.middle.svc.cluster.local.svc.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.00046184s
[INFO] 10.244.0.22:50432 - 31253 "A IN wordpress.middle.svc.cluster.local.svc.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000655558s
[INFO] 10.244.0.22:49093 - 33283 "AAAA IN wordpress.middle.svc.cluster.local.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000469041s
[INFO] 10.244.0.22:49093 - 48651 "A IN wordpress.middle.svc.cluster.local.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000535747s
[INFO] 10.244.0.22:37205 - 10545 "AAAA IN wordpress.middle.svc.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000417337s
[INFO] 10.244.0.22:37205 - 57400 "A IN wordpress.middle.svc.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000558849s


==> coredns [86ceb2801013] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:50728 - 28529 "HINFO IN 2737743628199859510.6669127984763918370. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.094117598s
[INFO] 10.244.0.14:41570 - 23960 "A IN mysql.database.svc.cluster.local.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.000231115s
[INFO] 10.244.0.14:45330 - 50580 "AAAA IN mysql.database.svc.cluster.local.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000504132s
[INFO] 10.244.0.14:41570 - 61083 "AAAA IN mysql.database.svc.cluster.local.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.000415427s
[INFO] 10.244.0.14:39541 - 22058 "AAAA IN mysql.database.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000105007s
[INFO] 10.244.0.14:45330 - 57489 "A IN mysql.database.svc.cluster.local.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.00014891s
[INFO] 10.244.0.14:37680 - 12368 "AAAA IN mysql.database.svc.cluster.local.middle.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.011047203s
[INFO] 10.244.0.14:37680 - 4947 "A IN mysql.database.svc.cluster.local.middle.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.011052704s
[INFO] 10.244.0.14:39541 - 58665 "A IN mysql.database.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000463729s
[INFO] 10.244.0.14:41335 - 59848 "A IN mysql.database.svc.cluster.local.middle.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.000195312s
[INFO] 10.244.0.14:35386 - 24265 "AAAA IN mysql.database.svc.cluster.local.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000102007s
[INFO] 10.244.0.14:41335 - 31989 "AAAA IN mysql.database.svc.cluster.local.middle.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.000563836s
[INFO] 10.244.0.14:35386 - 20683 "A IN mysql.database.svc.cluster.local.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000198413s
[INFO] 10.244.0.14:46163 - 8256 "AAAA IN mysql.database.svc.cluster.local.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.000097406s
[INFO] 10.244.0.14:46163 - 7746 "A IN mysql.database.svc.cluster.local.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.000175111s
[INFO] 10.244.0.14:53864 - 29585 "AAAA IN mysql.database.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000114907s
[INFO] 10.244.0.14:53864 - 4238 "A IN mysql.database.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000173612s
[INFO] 10.244.0.14:51319 - 19225 "AAAA IN mysql.database.svc.cluster.local.middle.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.008395531s
[INFO] 10.244.0.14:36057 - 12833 "AAAA IN mysql.database.svc.cluster.local.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.000146209s
[INFO] 10.244.0.14:51319 - 275 "A IN mysql.database.svc.cluster.local.middle.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.00837943s
[INFO] 10.244.0.14:36057 - 17981 "A IN mysql.database.svc.cluster.local.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.000268517s
[INFO] 10.244.0.14:49847 - 64419 "AAAA IN mysql.database.svc.cluster.local.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000240716s
[INFO] 10.244.0.14:49847 - 30104 "A IN mysql.database.svc.cluster.local.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000446628s
[INFO] 10.244.0.14:52574 - 22074 "AAAA IN mysql.database.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000614639s
[INFO] 10.244.0.14:52574 - 44614 "A IN mysql.database.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.001142072s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_10_25T11_23_40_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 25 Oct 2024 09:23:34 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 08 Nov 2024 08:20:50 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 08 Nov 2024 08:20:28 +0000   Fri, 25 Oct 2024 09:23:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 08 Nov 2024 08:20:28 +0000   Fri, 25 Oct 2024 09:23:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 08 Nov 2024 08:20:28 +0000   Fri, 25 Oct 2024 09:23:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 08 Nov 2024 08:20:28 +0000   Fri, 25 Oct 2024 09:23:35 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8052712Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8052712Ki
  pods:               110
System Info:
  Machine ID:                 2ed90581aad54b0992a398c147eb5f0f
  System UUID:                2ed90581aad54b0992a398c147eb5f0f
  Boot ID:                    d4148d1d-98c5-42af-8452-ca99abdb58fb
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---
  database                    mysql-0                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         13d
  default                     mysql-0                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         13d
  front                       nginx-deployment-6884d88685-cgh88    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13d
  front                       nginx-deployment-6884d88685-kv7qb    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13d
  front                       nginx-deployment-6884d88685-zmgnz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13d
  front                       nginx-dfddbd46c-4rdpl                0 (0%)        0 (0%)      0 (0%)           0 (0%)         39s
  kube-system                 coredns-6f6b679f8f-g7p7x             100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     13d
  kube-system                 etcd-minikube                        100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         13d
  kube-system                 kube-apiserver-minikube              250m (3%)     0 (0%)      0 (0%)           0 (0%)         13d
  kube-system                 kube-controller-manager-minikube     200m (2%)     0 (0%)      0 (0%)           0 (0%)         13d
  kube-system                 kube-proxy-zx86f                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         13d
  kube-system                 kube-scheduler-minikube              100m (1%)     0 (0%)      0 (0%)           0 (0%)         13d
  kube-system                 storage-provisioner                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         13d
  middle                      wordpress-6ddd454bcb-zmhlk           0 (0%)        0 (0%)      0 (0%)           0 (0%)         13d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           44m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  44m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           44m                kubelet          Starting kubelet.
  Warning  CgroupV1                           44m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced            44m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            44m (x7 over 44m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              44m (x7 over 44m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               44m (x7 over 44m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     44m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Nov 8 07:35] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3 #4 #5 #6 #7
[  +0.004024] PCI: Fatal: No config space access function found
[  +0.017214] PCI: System does not support PCI
[  +0.189016] kvm: already loaded the other module
[  +1.551230] FS-Cache: Duplicate cookie detected
[  +0.000436] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000541] FS-Cache: O-cookie d=000000003f5c2859{9P.session} n=00000000f150fdc2
[  +0.000612] FS-Cache: O-key=[10] '34323934393337343733'
[  +0.000422] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000524] FS-Cache: N-cookie d=000000003f5c2859{9P.session} n=000000009f3760e6
[  +0.000539] FS-Cache: N-key=[10] '34323934393337343733'
[  +0.982142] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000564] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000494] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000551] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000878] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000472] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000420] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000533] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.343037] Failed to connect to bus: No such file or directory
[  +0.102103] systemd-journald[51]: File /var/log/journal/060974753cc1445e89d51a23ea76bd62/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +7.157849] TCP: eth0: Driver has suspect GRO implementation, TCP performance may be compromised.
[ +13.111167] WSL (1) ERROR: ConfigApplyWindowsLibPath:2527: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.009325] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +0.078722] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000644] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000584] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000657] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001624] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000638] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000718] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000666] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.744129] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.002167] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.001093] WSL (1) ERROR: ConfigMountFsTab:2579: Processing fstab with mount -a failed.
[  +0.002047] WSL (1) ERROR: ConfigApplyWindowsLibPath:2527: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.003115] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001277] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.011544] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +0.119986] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000746] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000597] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001099] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001327] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.012487] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003771] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000765] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.625426] new mount options do not match the existing superblock, will be ignored
[  +0.000257] netlink: 'init': attribute type 4 has an invalid length.
[Nov 8 07:36] tmpfs: Unknown parameter 'noswap'


==> etcd [095666a4e48d] <==
{"level":"info","ts":"2024-11-08T07:36:28.213324Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-11-08T07:36:28.213384Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-11-08T07:36:28.215900Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-08T07:36:28.215731Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-08T07:36:28.216862Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-11-08T07:36:28.217124Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-11-08T07:36:34.018144Z","caller":"traceutil/trace.go:171","msg":"trace[364013368] transaction","detail":"{read_only:false; response_revision:13496; number_of_response:1; }","duration":"103.411758ms","start":"2024-11-08T07:36:33.914692Z","end":"2024-11-08T07:36:34.018104Z","steps":["trace[364013368] 'process raft request'  (duration: 96.666509ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T07:36:35.616073Z","caller":"traceutil/trace.go:171","msg":"trace[476768329] linearizableReadLoop","detail":"{readStateIndex:16805; appliedIndex:16804; }","duration":"108.861ms","start":"2024-11-08T07:36:35.507141Z","end":"2024-11-08T07:36:35.616002Z","steps":["trace[476768329] 'read index received'  (duration: 16.003812ms)","trace[476768329] 'applied index is now lower than readState.Index'  (duration: 92.854687ms)"],"step_count":2}
{"level":"info","ts":"2024-11-08T07:36:35.616353Z","caller":"traceutil/trace.go:171","msg":"trace[942713116] transaction","detail":"{read_only:false; response_revision:13503; number_of_response:1; }","duration":"110.448496ms","start":"2024-11-08T07:36:35.505874Z","end":"2024-11-08T07:36:35.616323Z","steps":["trace[942713116] 'process raft request'  (duration: 17.252738ms)","trace[942713116] 'compare'  (duration: 92.610465ms)"],"step_count":2}
{"level":"warn","ts":"2024-11-08T07:36:35.617169Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.988265ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/default/mysql-pv-storage-mysql-0\" ","response":"range_response_count:1 size:1239"}
{"level":"info","ts":"2024-11-08T07:36:35.617295Z","caller":"traceutil/trace.go:171","msg":"trace[1945102527] range","detail":"{range_begin:/registry/persistentvolumeclaims/default/mysql-pv-storage-mysql-0; range_end:; response_count:1; response_revision:13503; }","duration":"110.13734ms","start":"2024-11-08T07:36:35.507126Z","end":"2024-11-08T07:36:35.617263Z","steps":["trace[1945102527] 'agreement among raft nodes before linearized reading'  (duration: 109.594768ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T07:36:36.024362Z","caller":"traceutil/trace.go:171","msg":"trace[618756919] transaction","detail":"{read_only:false; response_revision:13507; number_of_response:1; }","duration":"104.792364ms","start":"2024-11-08T07:36:35.919509Z","end":"2024-11-08T07:36:36.024302Z","steps":["trace[618756919] 'process raft request'  (duration: 103.958346ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T07:36:36.117907Z","caller":"traceutil/trace.go:171","msg":"trace[966842152] transaction","detail":"{read_only:false; response_revision:13508; number_of_response:1; }","duration":"107.604771ms","start":"2024-11-08T07:36:36.010249Z","end":"2024-11-08T07:36:36.117854Z","steps":["trace[966842152] 'process raft request'  (duration: 95.081201ms)","trace[966842152] 'compare'  (duration: 11.894355ms)"],"step_count":2}
{"level":"info","ts":"2024-11-08T07:36:36.925222Z","caller":"traceutil/trace.go:171","msg":"trace[525903969] transaction","detail":"{read_only:false; response_revision:13521; number_of_response:1; }","duration":"101.476303ms","start":"2024-11-08T07:36:36.823682Z","end":"2024-11-08T07:36:36.925158Z","steps":["trace[525903969] 'process raft request'  (duration: 81.142123ms)","trace[525903969] 'compare'  (duration: 20.07445ms)"],"step_count":2}
{"level":"info","ts":"2024-11-08T07:36:39.105765Z","caller":"traceutil/trace.go:171","msg":"trace[1864552527] linearizableReadLoop","detail":"{readStateIndex:16858; appliedIndex:16857; }","duration":"100.061596ms","start":"2024-11-08T07:36:39.005660Z","end":"2024-11-08T07:36:39.105721Z","steps":["trace[1864552527] 'read index received'  (duration: 99.765547ms)","trace[1864552527] 'applied index is now lower than readState.Index'  (duration: 294.148¬µs)"],"step_count":2}
{"level":"info","ts":"2024-11-08T07:36:39.105995Z","caller":"traceutil/trace.go:171","msg":"trace[1717112778] transaction","detail":"{read_only:false; response_revision:13555; number_of_response:1; }","duration":"100.779354ms","start":"2024-11-08T07:36:39.005190Z","end":"2024-11-08T07:36:39.105970Z","steps":["trace[1717112778] 'process raft request'  (duration: 100.240084ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-08T07:36:39.106343Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.644888ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2024-11-08T07:36:39.106440Z","caller":"traceutil/trace.go:171","msg":"trace[1510480906] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:13555; }","duration":"100.765248ms","start":"2024-11-08T07:36:39.005649Z","end":"2024-11-08T07:36:39.106415Z","steps":["trace[1510480906] 'agreement among raft nodes before linearized reading'  (duration: 100.581556ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T07:36:39.120103Z","caller":"traceutil/trace.go:171","msg":"trace[1331801769] transaction","detail":"{read_only:false; response_revision:13556; number_of_response:1; }","duration":"100.428079ms","start":"2024-11-08T07:36:39.019627Z","end":"2024-11-08T07:36:39.120055Z","steps":["trace[1331801769] 'process raft request'  (duration: 99.859794ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-08T07:36:39.120835Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.863205ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-08T07:36:39.120937Z","caller":"traceutil/trace.go:171","msg":"trace[1694414814] range","detail":"{range_begin:/registry/minions; range_end:; response_count:0; response_revision:13556; }","duration":"113.985966ms","start":"2024-11-08T07:36:39.006923Z","end":"2024-11-08T07:36:39.120909Z","steps":["trace[1694414814] 'agreement among raft nodes before linearized reading'  (duration: 113.81478ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T07:36:39.421746Z","caller":"traceutil/trace.go:171","msg":"trace[1700449947] linearizableReadLoop","detail":"{readStateIndex:16862; appliedIndex:16860; }","duration":"106.450794ms","start":"2024-11-08T07:36:39.315253Z","end":"2024-11-08T07:36:39.421704Z","steps":["trace[1700449947] 'read index received'  (duration: 9.873243ms)","trace[1700449947] 'applied index is now lower than readState.Index'  (duration: 96.57595ms)"],"step_count":2}
{"level":"info","ts":"2024-11-08T07:36:39.421994Z","caller":"traceutil/trace.go:171","msg":"trace[1192866523] transaction","detail":"{read_only:false; response_revision:13559; number_of_response:1; }","duration":"110.414579ms","start":"2024-11-08T07:36:39.311552Z","end":"2024-11-08T07:36:39.421966Z","steps":["trace[1192866523] 'process raft request'  (duration: 93.396159ms)","trace[1192866523] 'compare'  (duration: 16.531976ms)"],"step_count":2}
{"level":"warn","ts":"2024-11-08T07:36:39.422681Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.696017ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/certificate-controller\" ","response":"range_response_count:1 size:209"}
{"level":"info","ts":"2024-11-08T07:36:39.422806Z","caller":"traceutil/trace.go:171","msg":"trace[480284352] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/certificate-controller; range_end:; response_count:1; response_revision:13559; }","duration":"106.834987ms","start":"2024-11-08T07:36:39.315936Z","end":"2024-11-08T07:36:39.422771Z","steps":["trace[480284352] 'agreement among raft nodes before linearized reading'  (duration: 106.312325ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-08T07:36:39.422933Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.658198ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/generic-garbage-collector\" ","response":"range_response_count:1 size:216"}
{"level":"info","ts":"2024-11-08T07:36:39.423027Z","caller":"traceutil/trace.go:171","msg":"trace[1983355469] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/generic-garbage-collector; range_end:; response_count:1; response_revision:13559; }","duration":"107.754047ms","start":"2024-11-08T07:36:39.315243Z","end":"2024-11-08T07:36:39.422997Z","steps":["trace[1983355469] 'agreement among raft nodes before linearized reading'  (duration: 107.493116ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-08T07:36:39.423240Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.094813ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-08T07:36:39.423318Z","caller":"traceutil/trace.go:171","msg":"trace[516063735] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13559; }","duration":"101.174953ms","start":"2024-11-08T07:36:39.322118Z","end":"2024-11-08T07:36:39.423293Z","steps":["trace[516063735] 'agreement among raft nodes before linearized reading'  (duration: 101.050991ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T07:36:40.105608Z","caller":"traceutil/trace.go:171","msg":"trace[1473186313] transaction","detail":"{read_only:false; response_revision:13565; number_of_response:1; }","duration":"179.245638ms","start":"2024-11-08T07:36:39.926319Z","end":"2024-11-08T07:36:40.105564Z","steps":["trace[1473186313] 'process raft request'  (duration: 87.657085ms)","trace[1473186313] 'compare'  (duration: 90.638877ms)"],"step_count":2}
{"level":"warn","ts":"2024-11-08T07:36:43.307777Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.364876ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/endpoint-controller\" ","response":"range_response_count:1 size:203"}
{"level":"info","ts":"2024-11-08T07:36:43.307985Z","caller":"traceutil/trace.go:171","msg":"trace[379766669] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/endpoint-controller; range_end:; response_count:1; response_revision:13580; }","duration":"101.559468ms","start":"2024-11-08T07:36:43.206361Z","end":"2024-11-08T07:36:43.307920Z","steps":["trace[379766669] 'agreement among raft nodes before linearized reading'  (duration: 99.423055ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T07:36:43.309202Z","caller":"traceutil/trace.go:171","msg":"trace[283385034] transaction","detail":"{read_only:false; response_revision:13578; number_of_response:1; }","duration":"103.068084ms","start":"2024-11-08T07:36:43.206077Z","end":"2024-11-08T07:36:43.309145Z","steps":["trace[283385034] 'process raft request'  (duration: 98.617373ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T07:36:44.305040Z","caller":"traceutil/trace.go:171","msg":"trace[686634173] transaction","detail":"{read_only:false; response_revision:13608; number_of_response:1; }","duration":"177.101997ms","start":"2024-11-08T07:36:44.127891Z","end":"2024-11-08T07:36:44.304993Z","steps":["trace[686634173] 'process raft request'  (duration: 176.681698ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T07:38:01.197726Z","caller":"traceutil/trace.go:171","msg":"trace[1409316760] transaction","detail":"{read_only:false; response_revision:13719; number_of_response:1; }","duration":"227.649342ms","start":"2024-11-08T07:38:00.970055Z","end":"2024-11-08T07:38:01.197705Z","steps":["trace[1409316760] 'process raft request'  (duration: 227.550008ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T07:46:29.826645Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13891}
{"level":"info","ts":"2024-11-08T07:46:29.863454Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":13891,"took":"35.798583ms","hash":1276616816,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1470464,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-11-08T07:46:29.863619Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1276616816,"revision":13891,"compact-revision":13063}
{"level":"info","ts":"2024-11-08T07:51:29.817382Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14132}
{"level":"info","ts":"2024-11-08T07:51:29.826328Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":14132,"took":"7.935057ms","hash":4126969321,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1716224,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-11-08T07:51:29.826508Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4126969321,"revision":14132,"compact-revision":13891}
{"level":"info","ts":"2024-11-08T07:56:29.810721Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14372}
{"level":"info","ts":"2024-11-08T07:56:29.820134Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":14372,"took":"8.667345ms","hash":3297041189,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1724416,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-11-08T07:56:29.820379Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3297041189,"revision":14372,"compact-revision":14132}
{"level":"info","ts":"2024-11-08T08:01:29.789267Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14613}
{"level":"info","ts":"2024-11-08T08:01:29.791843Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":14613,"took":"2.396835ms","hash":2882716532,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1716224,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-11-08T08:01:29.791888Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2882716532,"revision":14613,"compact-revision":14372}
{"level":"info","ts":"2024-11-08T08:03:51.029762Z","caller":"traceutil/trace.go:171","msg":"trace[623897577] transaction","detail":"{read_only:false; response_revision:14967; number_of_response:1; }","duration":"291.003241ms","start":"2024-11-08T08:03:50.738059Z","end":"2024-11-08T08:03:51.029062Z","steps":["trace[623897577] 'process raft request'  (duration: 290.387173ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T08:03:51.030114Z","caller":"traceutil/trace.go:171","msg":"trace[1965725051] linearizableReadLoop","detail":"{readStateIndex:18609; appliedIndex:18608; }","duration":"119.316608ms","start":"2024-11-08T08:03:50.909445Z","end":"2024-11-08T08:03:51.028761Z","steps":["trace[1965725051] 'read index received'  (duration: 119.060321ms)","trace[1965725051] 'applied index is now lower than readState.Index'  (duration: 250.487¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-11-08T08:03:51.033753Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.353454ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-08T08:03:51.034305Z","caller":"traceutil/trace.go:171","msg":"trace[1962165865] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14967; }","duration":"124.832926ms","start":"2024-11-08T08:03:50.909433Z","end":"2024-11-08T08:03:51.034266Z","steps":["trace[1962165865] 'agreement among raft nodes before linearized reading'  (duration: 120.05127ms)"],"step_count":1}
{"level":"info","ts":"2024-11-08T08:06:29.776770Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14853}
{"level":"info","ts":"2024-11-08T08:06:29.780612Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":14853,"took":"3.558914ms","hash":2931747149,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1699840,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-11-08T08:06:29.780664Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2931747149,"revision":14853,"compact-revision":14613}
{"level":"info","ts":"2024-11-08T08:11:29.776190Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15093}
{"level":"info","ts":"2024-11-08T08:11:29.784952Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":15093,"took":"8.214187ms","hash":975757349,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1712128,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-11-08T08:11:29.785151Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":975757349,"revision":15093,"compact-revision":14853}
{"level":"info","ts":"2024-11-08T08:16:29.761572Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15334}
{"level":"info","ts":"2024-11-08T08:16:29.766960Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":15334,"took":"4.909041ms","hash":2787768363,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1720320,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-11-08T08:16:29.767065Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2787768363,"revision":15334,"compact-revision":15093}


==> etcd [8e20331c4452] <==
{"level":"info","ts":"2024-10-25T14:40:27.172270Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11137}
{"level":"info","ts":"2024-10-25T14:40:27.179315Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":11137,"took":"6.4768ms","hash":4151991249,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1667072,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-10-25T14:40:27.179444Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4151991249,"revision":11137,"compact-revision":10897}
{"level":"info","ts":"2024-10-25T14:45:27.170175Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11376}
{"level":"info","ts":"2024-10-25T14:45:27.179271Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":11376,"took":"8.25893ms","hash":1251361111,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1667072,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-10-25T14:45:27.179472Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1251361111,"revision":11376,"compact-revision":11137}
{"level":"info","ts":"2024-10-25T14:50:27.175110Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11618}
{"level":"info","ts":"2024-10-25T14:50:27.185579Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":11618,"took":"9.342175ms","hash":2219480225,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1658880,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-10-25T14:50:27.185776Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2219480225,"revision":11618,"compact-revision":11376}
{"level":"info","ts":"2024-10-25T14:55:27.169669Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11862}
{"level":"info","ts":"2024-10-25T14:55:27.178918Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":11862,"took":"8.583796ms","hash":231626611,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1646592,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-10-25T14:55:27.179108Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":231626611,"revision":11862,"compact-revision":11618}
{"level":"info","ts":"2024-10-25T15:00:27.160118Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12102}
{"level":"info","ts":"2024-10-25T15:00:27.169437Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12102,"took":"8.783525ms","hash":1055969570,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1646592,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-10-25T15:00:27.169607Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1055969570,"revision":12102,"compact-revision":11862}
{"level":"warn","ts":"2024-10-25T15:04:41.603620Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-25T15:04:41.127369Z","time spent":"476.126868ms","remote":"127.0.0.1:45908","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-10-25T15:04:41.992060Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"247.655241ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032795841464938 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:12538 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128032795841464935 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"warn","ts":"2024-10-25T15:04:41.992715Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"267.996954ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-25T15:04:41.992702Z","caller":"traceutil/trace.go:171","msg":"trace[1595810635] linearizableReadLoop","detail":"{readStateIndex:15594; appliedIndex:15593; }","duration":"267.543016ms","start":"2024-10-25T15:04:41.724723Z","end":"2024-10-25T15:04:41.992266Z","steps":["trace[1595810635] 'read index received'  (duration: 42.104¬µs)","trace[1595810635] 'applied index is now lower than readState.Index'  (duration: 267.499611ms)"],"step_count":2}
{"level":"info","ts":"2024-10-25T15:04:41.992788Z","caller":"traceutil/trace.go:171","msg":"trace[992719785] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12546; }","duration":"268.07136ms","start":"2024-10-25T15:04:41.724699Z","end":"2024-10-25T15:04:41.992770Z","steps":["trace[992719785] 'agreement among raft nodes before linearized reading'  (duration: 267.71143ms)"],"step_count":1}
{"level":"info","ts":"2024-10-25T15:04:41.993167Z","caller":"traceutil/trace.go:171","msg":"trace[1313340886] transaction","detail":"{read_only:false; response_revision:12546; number_of_response:1; }","duration":"387.907045ms","start":"2024-10-25T15:04:41.604397Z","end":"2024-10-25T15:04:41.992304Z","steps":["trace[1313340886] 'process raft request'  (duration: 119.89729ms)","trace[1313340886] 'compare'  (duration: 246.762967ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-25T15:04:41.993300Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-25T15:04:41.604340Z","time spent":"388.91573ms","remote":"127.0.0.1:45908","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:12538 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128032795841464935 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"info","ts":"2024-10-25T15:05:27.146698Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12343}
{"level":"info","ts":"2024-10-25T15:05:27.153276Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12343,"took":"6.210123ms","hash":3029438133,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1667072,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-10-25T15:05:27.153412Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3029438133,"revision":12343,"compact-revision":12102}
{"level":"info","ts":"2024-10-25T15:10:27.163291Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12584}
{"level":"info","ts":"2024-10-25T15:10:27.172401Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12584,"took":"8.399862ms","hash":2492833226,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1667072,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-10-25T15:10:27.172588Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2492833226,"revision":12584,"compact-revision":12343}
{"level":"info","ts":"2024-10-25T15:15:27.160698Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12824}
{"level":"info","ts":"2024-10-25T15:15:27.169049Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12824,"took":"7.808969ms","hash":3006453987,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1658880,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-10-25T15:15:27.169207Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3006453987,"revision":12824,"compact-revision":12584}
{"level":"info","ts":"2024-10-25T15:17:25.301354Z","caller":"traceutil/trace.go:171","msg":"trace[1608072402] transaction","detail":"{read_only:false; response_revision:13158; number_of_response:1; }","duration":"124.004143ms","start":"2024-10-25T15:17:25.177285Z","end":"2024-10-25T15:17:25.301289Z","steps":["trace[1608072402] 'process raft request'  (duration: 123.851331ms)"],"step_count":1}
{"level":"info","ts":"2024-10-25T15:17:57.666283Z","caller":"traceutil/trace.go:171","msg":"trace[547238093] transaction","detail":"{read_only:false; response_revision:13183; number_of_response:1; }","duration":"112.052716ms","start":"2024-10-25T15:17:57.554210Z","end":"2024-10-25T15:17:57.666263Z","steps":["trace[547238093] 'process raft request'  (duration: 111.910504ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-25T15:17:58.175563Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"171.650165ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-25T15:17:58.175622Z","caller":"traceutil/trace.go:171","msg":"trace[820578533] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13183; }","duration":"172.006994ms","start":"2024-10-25T15:17:58.003604Z","end":"2024-10-25T15:17:58.175611Z","steps":["trace[820578533] 'range keys from in-memory index tree'  (duration: 171.638164ms)"],"step_count":1}
{"level":"info","ts":"2024-10-25T15:17:59.820625Z","caller":"traceutil/trace.go:171","msg":"trace[1721567759] transaction","detail":"{read_only:false; response_revision:13184; number_of_response:1; }","duration":"148.10925ms","start":"2024-10-25T15:17:59.672497Z","end":"2024-10-25T15:17:59.820606Z","steps":["trace[1721567759] 'process raft request'  (duration: 147.98444ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-25T15:18:01.566039Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-25T15:18:01.115928Z","time spent":"450.107319ms","remote":"127.0.0.1:45908","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2024-10-25T15:18:01.568452Z","caller":"traceutil/trace.go:171","msg":"trace[1333834658] transaction","detail":"{read_only:false; response_revision:13185; number_of_response:1; }","duration":"107.17332ms","start":"2024-10-25T15:18:01.461263Z","end":"2024-10-25T15:18:01.568436Z","steps":["trace[1333834658] 'process raft request'  (duration: 107.011306ms)"],"step_count":1}
{"level":"info","ts":"2024-10-25T15:18:04.194234Z","caller":"traceutil/trace.go:171","msg":"trace[962234718] linearizableReadLoop","detail":"{readStateIndex:16403; appliedIndex:16402; }","duration":"190.14817ms","start":"2024-10-25T15:18:04.004068Z","end":"2024-10-25T15:18:04.194217Z","steps":["trace[962234718] 'read index received'  (duration: 190.081164ms)","trace[962234718] 'applied index is now lower than readState.Index'  (duration: 66.406¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-10-25T15:18:04.194432Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.352687ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-25T15:18:04.194408Z","caller":"traceutil/trace.go:171","msg":"trace[2006735544] transaction","detail":"{read_only:false; response_revision:13188; number_of_response:1; }","duration":"265.507601ms","start":"2024-10-25T15:18:03.928888Z","end":"2024-10-25T15:18:04.194395Z","steps":["trace[2006735544] 'process raft request'  (duration: 265.235779ms)"],"step_count":1}
{"level":"info","ts":"2024-10-25T15:18:04.194459Z","caller":"traceutil/trace.go:171","msg":"trace[362520417] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13188; }","duration":"190.38729ms","start":"2024-10-25T15:18:04.004063Z","end":"2024-10-25T15:18:04.194450Z","steps":["trace[362520417] 'agreement among raft nodes before linearized reading'  (duration: 190.338386ms)"],"step_count":1}
{"level":"info","ts":"2024-10-25T15:18:05.971883Z","caller":"traceutil/trace.go:171","msg":"trace[569844409] transaction","detail":"{read_only:false; response_revision:13189; number_of_response:1; }","duration":"137.973381ms","start":"2024-10-25T15:18:05.833895Z","end":"2024-10-25T15:18:05.971868Z","steps":["trace[569844409] 'process raft request'  (duration: 137.851871ms)"],"step_count":1}
{"level":"info","ts":"2024-10-25T15:20:27.150466Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13063}
{"level":"info","ts":"2024-10-25T15:20:27.156631Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":13063,"took":"5.786176ms","hash":31215112,"current-db-size-bytes":3170304,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1646592,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-10-25T15:20:27.156742Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":31215112,"revision":13063,"compact-revision":12824}
{"level":"warn","ts":"2024-10-25T15:24:22.113339Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.002688151s","expected-duration":"1s"}
{"level":"info","ts":"2024-10-25T15:24:22.144296Z","caller":"traceutil/trace.go:171","msg":"trace[1603713879] linearizableReadLoop","detail":"{readStateIndex:16785; appliedIndex:16784; }","duration":"508.679102ms","start":"2024-10-25T15:24:21.635591Z","end":"2024-10-25T15:24:22.144270Z","steps":["trace[1603713879] 'read index received'  (duration: 494.593503ms)","trace[1603713879] 'applied index is now lower than readState.Index'  (duration: 14.083299ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-25T15:24:22.144329Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-25T15:24:21.112587Z","time spent":"1.033737833s","remote":"127.0.0.1:45908","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-10-25T15:24:22.150030Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"171.498644ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-25T15:24:22.150246Z","caller":"traceutil/trace.go:171","msg":"trace[814380854] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13489; }","duration":"177.211708ms","start":"2024-10-25T15:24:21.972941Z","end":"2024-10-25T15:24:22.150153Z","steps":["trace[814380854] 'agreement among raft nodes before linearized reading'  (duration: 171.486843ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-25T15:24:22.152870Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"508.811211ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-25T15:24:22.152985Z","caller":"traceutil/trace.go:171","msg":"trace[2075083735] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13489; }","duration":"517.378157ms","start":"2024-10-25T15:24:21.635579Z","end":"2024-10-25T15:24:22.152957Z","steps":["trace[2075083735] 'agreement among raft nodes before linearized reading'  (duration: 508.783009ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-25T15:24:22.153313Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-25T15:24:21.635478Z","time spent":"517.806986ms","remote":"127.0.0.1:45874","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-10-25T15:24:22.626746Z","caller":"traceutil/trace.go:171","msg":"trace[1696204613] linearizableReadLoop","detail":"{readStateIndex:16786; appliedIndex:16785; }","duration":"472.47955ms","start":"2024-10-25T15:24:22.154221Z","end":"2024-10-25T15:24:22.626700Z","steps":["trace[1696204613] 'read index received'  (duration: 439.653555ms)","trace[1696204613] 'applied index is now lower than readState.Index'  (duration: 32.823894ms)"],"step_count":2}
{"level":"info","ts":"2024-10-25T15:24:22.626805Z","caller":"traceutil/trace.go:171","msg":"trace[1944182657] transaction","detail":"{read_only:false; response_revision:13490; number_of_response:1; }","duration":"481.401819ms","start":"2024-10-25T15:24:22.145361Z","end":"2024-10-25T15:24:22.626763Z","steps":["trace[1944182657] 'process raft request'  (duration: 448.415014ms)","trace[1944182657] 'compare'  (duration: 32.713287ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-25T15:24:22.627052Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"472.787369ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-25T15:24:22.627225Z","caller":"traceutil/trace.go:171","msg":"trace[792880946] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13490; }","duration":"472.968881ms","start":"2024-10-25T15:24:22.154217Z","end":"2024-10-25T15:24:22.627185Z","steps":["trace[792880946] 'agreement among raft nodes before linearized reading'  (duration: 472.637159ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-25T15:24:22.646081Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-25T15:24:22.154184Z","time spent":"491.831084ms","remote":"127.0.0.1:45886","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-10-25T15:24:22.739885Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-25T15:24:22.145345Z","time spent":"481.807444ms","remote":"127.0.0.1:45908","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:13482 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128032795841471053 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}


==> kernel <==
 08:20:51 up 45 min,  0 users,  load average: 0.66, 0.55, 0.43
Linux minikube 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [28c92e88ff37] <==
I1108 07:36:32.646503       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1108 07:36:32.646635       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1108 07:36:33.480034       1 secure_serving.go:213] Serving securely on [::]:8443
I1108 07:36:33.480789       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1108 07:36:33.480936       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1108 07:36:33.485653       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1108 07:36:33.485693       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1108 07:36:33.485756       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1108 07:36:33.485809       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1108 07:36:33.485710       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1108 07:36:33.486063       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1108 07:36:33.486226       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1108 07:36:33.486398       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1108 07:36:33.487603       1 controller.go:119] Starting legacy_token_tracking_controller
I1108 07:36:33.487700       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1108 07:36:33.487806       1 controller.go:142] Starting OpenAPI controller
I1108 07:36:33.487874       1 controller.go:90] Starting OpenAPI V3 controller
I1108 07:36:33.487911       1 naming_controller.go:294] Starting NamingConditionController
I1108 07:36:33.487967       1 establishing_controller.go:81] Starting EstablishingController
I1108 07:36:33.488029       1 aggregator.go:169] waiting for initial CRD sync...
I1108 07:36:33.488065       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1108 07:36:33.488081       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1108 07:36:33.488108       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1108 07:36:33.488122       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1108 07:36:33.488155       1 crd_finalizer.go:269] Starting CRDFinalizer
I1108 07:36:33.488274       1 local_available_controller.go:156] Starting LocalAvailability controller
I1108 07:36:33.488307       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1108 07:36:33.488399       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1108 07:36:33.488420       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1108 07:36:33.493759       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1108 07:36:33.495378       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1108 07:36:33.495508       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1108 07:36:33.496611       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1108 07:36:33.496761       1 controller.go:78] Starting OpenAPI AggregationController
I1108 07:36:33.497653       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1108 07:36:33.497865       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1108 07:36:33.607185       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1108 07:36:33.805196       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1108 07:36:33.805379       1 policy_source.go:224] refreshing policies
I1108 07:36:33.806755       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1108 07:36:33.806855       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1108 07:36:33.905164       1 shared_informer.go:320] Caches are synced for node_authorizer
I1108 07:36:33.905359       1 shared_informer.go:320] Caches are synced for configmaps
I1108 07:36:33.907734       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1108 07:36:33.908575       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1108 07:36:33.909328       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1108 07:36:33.909343       1 cache.go:39] Caches are synced for LocalAvailability controller
I1108 07:36:33.911128       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1108 07:36:33.911195       1 aggregator.go:171] initial CRD sync complete...
I1108 07:36:33.911216       1 autoregister_controller.go:144] Starting autoregister controller
I1108 07:36:33.911231       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1108 07:36:33.911247       1 cache.go:39] Caches are synced for autoregister controller
I1108 07:36:34.011777       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
E1108 07:36:34.107256       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1108 07:36:34.515956       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1108 07:36:43.130462       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1108 07:36:43.424264       1 controller.go:615] quota admission added evaluator for: endpoints
E1108 07:36:44.109536       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: bcc740a2-a58a-4aa7-a148-008b75a9ea27, UID in object meta: "
I1108 08:20:12.007664       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1108 08:20:12.053031       1 controller.go:615] quota admission added evaluator for: replicasets.apps


==> kube-apiserver [86e33c9aa8f2] <==
I1025 11:45:29.115526       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1025 11:45:29.115575       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1025 11:45:29.113838       1 controller.go:78] Starting OpenAPI AggregationController
I1025 11:45:29.113849       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1025 11:45:29.156289       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1025 11:45:29.113870       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1025 11:45:29.156327       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1025 11:45:29.113908       1 controller.go:119] Starting legacy_token_tracking_controller
I1025 11:45:29.156354       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1025 11:45:29.114113       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1025 11:45:29.114300       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1025 11:45:29.158222       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1025 11:45:29.114318       1 controller.go:90] Starting OpenAPI V3 controller
I1025 11:45:29.114319       1 controller.go:142] Starting OpenAPI controller
I1025 11:45:29.114326       1 naming_controller.go:294] Starting NamingConditionController
I1025 11:45:29.114382       1 establishing_controller.go:81] Starting EstablishingController
I1025 11:45:29.114691       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1025 11:45:29.114699       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1025 11:45:29.114710       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1025 11:45:29.114786       1 crd_finalizer.go:269] Starting CRDFinalizer
I1025 11:45:29.114808       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1025 11:45:29.159253       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1025 11:45:29.382859       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1025 11:45:29.383052       1 policy_source.go:224] refreshing policies
I1025 11:45:29.456623       1 cache.go:39] Caches are synced for LocalAvailability controller
I1025 11:45:29.457481       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1025 11:45:29.457792       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1025 11:45:29.457847       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1025 11:45:29.457986       1 aggregator.go:171] initial CRD sync complete...
I1025 11:45:29.458243       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1025 11:45:29.459294       1 shared_informer.go:320] Caches are synced for configmaps
I1025 11:45:29.459304       1 autoregister_controller.go:144] Starting autoregister controller
I1025 11:45:29.459477       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1025 11:45:29.459489       1 cache.go:39] Caches are synced for autoregister controller
I1025 11:45:29.462490       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1025 11:45:29.463361       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1025 11:45:29.466836       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1025 11:45:29.474982       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1025 11:45:29.556346       1 shared_informer.go:320] Caches are synced for node_authorizer
E1025 11:45:29.660587       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1025 11:45:30.165672       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1025 11:45:32.735982       1 controller.go:615] quota admission added evaluator for: endpoints
I1025 11:45:32.785585       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1025 11:58:33.996253       1 controller.go:615] quota admission added evaluator for: statefulsets.apps
I1025 11:58:34.057695       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I1025 12:05:27.326592       1 alloc.go:330] "allocated clusterIPs" service="default/mysql-service" clusterIPs={"IPv4":"10.103.156.202"}
E1025 12:18:51.117733       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:38104: use of closed network connection
I1025 12:34:13.564406       1 controller.go:615] quota admission added evaluator for: namespaces
I1025 12:55:53.742280       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1025 12:55:53.772652       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1025 13:04:01.078736       1 alloc.go:330] "allocated clusterIPs" service="default/wordpress-service" clusterIPs={"IPv4":"10.99.27.252"}
E1025 13:05:17.233915       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:56814: use of closed network connection
I1025 13:20:31.079796       1 alloc.go:330] "allocated clusterIPs" service="database/mysql-service" clusterIPs={"IPv4":"10.96.7.25"}
I1025 13:21:41.190559       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1025 13:21:41.201675       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1025 13:24:19.418661       1 alloc.go:330] "allocated clusterIPs" service="middle/wordpress-service" clusterIPs={"IPv4":"10.110.181.97"}
E1025 13:51:35.789828       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:44300: use of closed network connection
E1025 14:50:29.301864       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:55288: use of closed network connection
E1025 14:55:26.221473       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:43768: use of closed network connection
E1025 14:55:33.758186       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:43784: use of closed network connection


==> kube-controller-manager [1b61b93ae330] <==
I1108 07:36:43.016715       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1108 07:36:43.020129       1 shared_informer.go:320] Caches are synced for taint
I1108 07:36:43.020721       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1108 07:36:43.021076       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1108 07:36:43.021266       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1108 07:36:43.025602       1 shared_informer.go:320] Caches are synced for persistent volume
I1108 07:36:43.025869       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1108 07:36:43.028180       1 shared_informer.go:320] Caches are synced for HPA
I1108 07:36:43.028246       1 shared_informer.go:320] Caches are synced for deployment
I1108 07:36:43.104898       1 shared_informer.go:320] Caches are synced for endpoint
I1108 07:36:43.106275       1 shared_informer.go:320] Caches are synced for service account
I1108 07:36:43.106348       1 shared_informer.go:320] Caches are synced for job
I1108 07:36:43.108104       1 shared_informer.go:320] Caches are synced for cronjob
I1108 07:36:43.108274       1 shared_informer.go:320] Caches are synced for disruption
I1108 07:36:43.108303       1 shared_informer.go:320] Caches are synced for stateful set
I1108 07:36:43.124285       1 shared_informer.go:320] Caches are synced for namespace
I1108 07:36:43.124432       1 shared_informer.go:320] Caches are synced for TTL after finished
I1108 07:36:43.124491       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1108 07:36:43.127339       1 shared_informer.go:320] Caches are synced for resource quota
I1108 07:36:43.127655       1 shared_informer.go:320] Caches are synced for resource quota
I1108 07:36:43.508438       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="488.35442ms"
I1108 07:36:43.511874       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-deployment-6884d88685" duration="494.915032ms"
I1108 07:36:43.520798       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-deployment-6884d88685" duration="119.957¬µs"
I1108 07:36:43.521068       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="89.943¬µs"
I1108 07:36:43.607810       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="589.616348ms"
I1108 07:36:43.608857       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="150.871¬µs"
I1108 07:36:43.611837       1 shared_informer.go:320] Caches are synced for garbage collector
I1108 07:36:43.611887       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1108 07:36:43.618809       1 shared_informer.go:320] Caches are synced for garbage collector
I1108 07:36:45.974862       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-deployment-6884d88685" duration="7.375098ms"
I1108 07:36:45.974956       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-deployment-6884d88685" duration="36.217¬µs"
I1108 07:36:46.020638       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-deployment-6884d88685" duration="7.318671ms"
I1108 07:36:46.020718       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-deployment-6884d88685" duration="32.315¬µs"
I1108 07:36:46.031266       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="75.936¬µs"
I1108 07:36:46.124613       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-deployment-6884d88685" duration="6.351412ms"
I1108 07:36:46.124694       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-deployment-6884d88685" duration="32.515¬µs"
I1108 07:37:04.560478       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="17.798377ms"
I1108 07:37:04.560590       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="54.173¬µs"
I1108 07:37:15.722834       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="11.064686ms"
I1108 07:37:15.723042       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="54.863¬µs"
I1108 07:37:25.383184       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1108 07:42:32.235941       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1108 07:47:38.309054       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1108 07:52:43.863461       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1108 07:57:50.049348       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1108 08:02:55.174890       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1108 08:08:01.433584       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1108 08:13:08.323482       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1108 08:18:15.198534       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1108 08:20:12.095037       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-dfddbd46c" duration="33.727319ms"
I1108 08:20:12.101689       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-dfddbd46c" duration="6.515802ms"
I1108 08:20:12.101846       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-dfddbd46c" duration="50.604¬µs"
I1108 08:20:12.126610       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-dfddbd46c" duration="46.504¬µs"
I1108 08:20:12.146407       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-dfddbd46c" duration="106.21¬µs"
I1108 08:20:21.134868       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-dfddbd46c" duration="144.814¬µs"
I1108 08:20:23.214626       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-dfddbd46c" duration="72.107¬µs"
I1108 08:20:24.293949       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-dfddbd46c" duration="670.345¬µs"
I1108 08:20:28.168380       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1108 08:20:38.729402       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-dfddbd46c" duration="545.848¬µs"
I1108 08:20:51.450096       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="front/nginx-dfddbd46c" duration="76.107¬µs"


==> kube-controller-manager [b336c460ee4c] <==
I1025 12:55:53.832408       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="32.502¬µs"
I1025 13:00:42.933828       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 13:05:51.603186       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 13:10:57.342236       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 13:16:04.340381       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 13:21:10.816342       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 13:26:17.131565       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 13:31:23.813405       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 13:36:29.535856       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 13:41:35.920151       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
E1025 13:42:04.981808       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
I1025 13:42:04.990321       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="278.619¬µs"
E1025 13:42:17.618676       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:42:32.618075       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:42:47.618336       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:43:02.617649       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:43:17.618648       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:43:32.617068       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:43:47.618004       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:44:02.617351       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:44:17.618331       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:44:32.617152       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:44:47.617499       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:45:02.617028       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:45:17.616796       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:45:32.614592       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:45:47.615832       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:46:02.613585       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:46:17.614138       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:46:32.614160       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
I1025 13:46:42.460682       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
E1025 13:46:47.615044       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:47:02.613421       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:47:17.614377       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
E1025 13:47:23.564917       1 pv_controller.go:1586] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="middle/wordpress-pvc"
I1025 13:47:23.575969       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="84.906¬µs"
I1025 13:47:33.834124       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="126.708¬µs"
I1025 13:47:44.696767       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="274.717¬µs"
I1025 13:47:44.778659       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="257.716¬µs"
I1025 13:48:15.318484       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="34.552092ms"
I1025 13:48:15.318580       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="middle/wordpress-6ddd454bcb" duration="38.603¬µs"
I1025 13:48:33.891631       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 13:53:39.928838       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 13:58:46.386914       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:03:51.944507       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:08:58.670691       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:14:04.409611       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:19:09.722920       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:24:15.709164       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:29:21.985589       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:34:27.805451       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:39:35.225580       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:44:40.873152       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:49:47.867356       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:54:53.262904       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 14:59:59.903924       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 15:05:06.660282       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 15:10:12.806245       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 15:15:19.506325       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1025 15:20:26.826025       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [71fe33837191] <==
E1025 11:45:21.959544       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1025 11:45:21.968191       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1025 11:45:22.284030       1 server_linux.go:66] "Using iptables proxy"
E1025 11:45:23.259471       1 server.go:666] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E1025 11:45:29.468749       1 server.go:666] "Failed to retrieve node info" err="nodes \"minikube\" is forbidden: User \"system:serviceaccount:kube-system:kube-proxy\" cannot get resource \"nodes\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:service-account-issuer-discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:node-proxier\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found]"
I1025 11:45:31.859050       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1025 11:45:31.859403       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1025 11:45:31.909741       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1025 11:45:31.909935       1 server_linux.go:169] "Using iptables Proxier"
I1025 11:45:31.913637       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1025 11:45:31.917263       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1025 11:45:31.919792       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1025 11:45:31.921968       1 server.go:483] "Version info" version="v1.31.0"
I1025 11:45:31.922059       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1025 11:45:31.928859       1 config.go:197] "Starting service config controller"
I1025 11:45:31.929254       1 config.go:326] "Starting node config controller"
I1025 11:45:31.929518       1 shared_informer.go:313] Waiting for caches to sync for service config
I1025 11:45:31.929561       1 shared_informer.go:313] Waiting for caches to sync for node config
I1025 11:45:31.929301       1 config.go:104] "Starting endpoint slice config controller"
I1025 11:45:31.929809       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1025 11:45:32.029826       1 shared_informer.go:320] Caches are synced for node config
I1025 11:45:32.029900       1 shared_informer.go:320] Caches are synced for service config
I1025 11:45:32.029917       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [e81197fb7cd9] <==
E1108 07:36:44.839988       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1108 07:36:44.841055       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1108 07:36:44.928283       1 server_linux.go:66] "Using iptables proxy"
I1108 07:36:45.263445       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1108 07:36:45.263561       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1108 07:36:45.293798       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1108 07:36:45.293897       1 server_linux.go:169] "Using iptables Proxier"
I1108 07:36:45.295581       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1108 07:36:45.296562       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1108 07:36:45.297489       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1108 07:36:45.297640       1 server.go:483] "Version info" version="v1.31.0"
I1108 07:36:45.297667       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1108 07:36:45.300305       1 config.go:326] "Starting node config controller"
I1108 07:36:45.300332       1 config.go:104] "Starting endpoint slice config controller"
I1108 07:36:45.300610       1 config.go:197] "Starting service config controller"
I1108 07:36:45.301729       1 shared_informer.go:313] Waiting for caches to sync for node config
I1108 07:36:45.301735       1 shared_informer.go:313] Waiting for caches to sync for service config
I1108 07:36:45.301788       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1108 07:36:45.402629       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1108 07:36:45.402688       1 shared_informer.go:320] Caches are synced for node config
I1108 07:36:45.402698       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [406b867db49f] <==
I1108 07:36:29.804185       1 serving.go:386] Generated self-signed cert in-memory
W1108 07:36:33.716036       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1108 07:36:33.716202       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1108 07:36:33.716282       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1108 07:36:33.716314       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1108 07:36:34.105175       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1108 07:36:34.105267       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1108 07:36:34.115929       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1108 07:36:34.116934       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1108 07:36:34.117133       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1108 07:36:34.126574       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1108 07:36:34.228396       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [c749d1f9e09d] <==
I1025 11:45:24.373272       1 serving.go:386] Generated self-signed cert in-memory
W1025 11:45:29.181618       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1025 11:45:29.181744       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1025 11:45:29.181771       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1025 11:45:29.181948       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1025 11:45:29.562802       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1025 11:45:29.562978       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1025 11:45:29.578628       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1025 11:45:29.578948       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1025 11:45:29.579141       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1025 11:45:29.580326       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1025 11:45:29.681630       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E1025 13:47:33.823041       1 schedule_one.go:160] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"middle/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"middle/wordpress-pvc\"" pod="middle/wordpress-6ddd454bcb-zmhlk"
E1025 13:47:33.823985       1 schedule_one.go:1057] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"middle/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"middle/wordpress-pvc\"" pod="middle/wordpress-6ddd454bcb-zmhlk"


==> kubelet <==
Nov 08 08:16:05 minikube kubelet[1689]: E1108 08:16:05.472897    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:16:19 minikube kubelet[1689]: E1108 08:16:19.466080    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:16:19 minikube kubelet[1689]: E1108 08:16:19.467270    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:16:31 minikube kubelet[1689]: E1108 08:16:31.463857    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:16:31 minikube kubelet[1689]: E1108 08:16:31.465073    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:16:46 minikube kubelet[1689]: E1108 08:16:46.469923    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:16:46 minikube kubelet[1689]: E1108 08:16:46.471727    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:16:58 minikube kubelet[1689]: E1108 08:16:58.460605    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:16:58 minikube kubelet[1689]: E1108 08:16:58.462059    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:17:11 minikube kubelet[1689]: E1108 08:17:11.467134    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:17:11 minikube kubelet[1689]: E1108 08:17:11.468715    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:17:26 minikube kubelet[1689]: E1108 08:17:26.458433    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:17:26 minikube kubelet[1689]: E1108 08:17:26.459741    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:17:37 minikube kubelet[1689]: E1108 08:17:37.465433    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:17:37 minikube kubelet[1689]: E1108 08:17:37.467125    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:17:48 minikube kubelet[1689]: E1108 08:17:48.458253    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:17:48 minikube kubelet[1689]: E1108 08:17:48.459466    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:18:01 minikube kubelet[1689]: E1108 08:18:01.460785    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:18:01 minikube kubelet[1689]: E1108 08:18:01.462941    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:18:15 minikube kubelet[1689]: E1108 08:18:15.462016    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:18:15 minikube kubelet[1689]: E1108 08:18:15.464083    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:18:28 minikube kubelet[1689]: E1108 08:18:28.451818    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:18:28 minikube kubelet[1689]: E1108 08:18:28.453085    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:18:40 minikube kubelet[1689]: E1108 08:18:40.450846    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:18:40 minikube kubelet[1689]: E1108 08:18:40.452111    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:18:52 minikube kubelet[1689]: E1108 08:18:52.456649    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:18:52 minikube kubelet[1689]: E1108 08:18:52.458538    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:19:05 minikube kubelet[1689]: E1108 08:19:05.450083    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:19:05 minikube kubelet[1689]: E1108 08:19:05.451362    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:19:20 minikube kubelet[1689]: E1108 08:19:20.451842    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:19:20 minikube kubelet[1689]: E1108 08:19:20.453281    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:19:34 minikube kubelet[1689]: E1108 08:19:34.448594    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:19:34 minikube kubelet[1689]: E1108 08:19:34.449877    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:19:45 minikube kubelet[1689]: E1108 08:19:45.452903    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:19:45 minikube kubelet[1689]: E1108 08:19:45.454687    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:19:58 minikube kubelet[1689]: E1108 08:19:58.444287    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:19:58 minikube kubelet[1689]: E1108 08:19:58.445557    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:20:10 minikube kubelet[1689]: E1108 08:20:10.449673    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:20:10 minikube kubelet[1689]: E1108 08:20:10.452228    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:20:12 minikube kubelet[1689]: I1108 08:20:12.212228    1689 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"nginx-config-volume\" (UniqueName: \"kubernetes.io/configmap/2d757e66-ca2a-43e3-b429-4ae803d7ccf1-nginx-config-volume\") pod \"nginx-dfddbd46c-4rdpl\" (UID: \"2d757e66-ca2a-43e3-b429-4ae803d7ccf1\") " pod="front/nginx-dfddbd46c-4rdpl"
Nov 08 08:20:12 minikube kubelet[1689]: I1108 08:20:12.212343    1689 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dntql\" (UniqueName: \"kubernetes.io/projected/2d757e66-ca2a-43e3-b429-4ae803d7ccf1-kube-api-access-dntql\") pod \"nginx-dfddbd46c-4rdpl\" (UID: \"2d757e66-ca2a-43e3-b429-4ae803d7ccf1\") " pod="front/nginx-dfddbd46c-4rdpl"
Nov 08 08:20:13 minikube kubelet[1689]: I1108 08:20:13.860696    1689 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7e9e043bece60d8e5e5cb9548504f261de2ffb483257debc636e4e7293161c15"
Nov 08 08:20:21 minikube kubelet[1689]: I1108 08:20:21.057814    1689 scope.go:117] "RemoveContainer" containerID="738121fcb0ac50945c6cd2df0154badf7f8a2347c91009aaa35647004f7126f7"
Nov 08 08:20:23 minikube kubelet[1689]: I1108 08:20:23.195846    1689 scope.go:117] "RemoveContainer" containerID="738121fcb0ac50945c6cd2df0154badf7f8a2347c91009aaa35647004f7126f7"
Nov 08 08:20:23 minikube kubelet[1689]: I1108 08:20:23.196567    1689 scope.go:117] "RemoveContainer" containerID="06c8691532f273b185a26b85e3fd6ebc40b1f445e6de840bae5440545c76ba66"
Nov 08 08:20:23 minikube kubelet[1689]: E1108 08:20:23.197177    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with CrashLoopBackOff: \"back-off 10s restarting failed container=nginx pod=nginx-dfddbd46c-4rdpl_front(2d757e66-ca2a-43e3-b429-4ae803d7ccf1)\"" pod="front/nginx-dfddbd46c-4rdpl" podUID="2d757e66-ca2a-43e3-b429-4ae803d7ccf1"
Nov 08 08:20:24 minikube kubelet[1689]: I1108 08:20:24.264143    1689 scope.go:117] "RemoveContainer" containerID="06c8691532f273b185a26b85e3fd6ebc40b1f445e6de840bae5440545c76ba66"
Nov 08 08:20:24 minikube kubelet[1689]: E1108 08:20:24.264639    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with CrashLoopBackOff: \"back-off 10s restarting failed container=nginx pod=nginx-dfddbd46c-4rdpl_front(2d757e66-ca2a-43e3-b429-4ae803d7ccf1)\"" pod="front/nginx-dfddbd46c-4rdpl" podUID="2d757e66-ca2a-43e3-b429-4ae803d7ccf1"
Nov 08 08:20:25 minikube kubelet[1689]: E1108 08:20:25.459348    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:20:25 minikube kubelet[1689]: E1108 08:20:25.461170    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:20:36 minikube kubelet[1689]: I1108 08:20:36.441536    1689 scope.go:117] "RemoveContainer" containerID="06c8691532f273b185a26b85e3fd6ebc40b1f445e6de840bae5440545c76ba66"
Nov 08 08:20:36 minikube kubelet[1689]: E1108 08:20:36.447159    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:20:36 minikube kubelet[1689]: E1108 08:20:36.448838    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:20:38 minikube kubelet[1689]: I1108 08:20:38.697563    1689 scope.go:117] "RemoveContainer" containerID="06c8691532f273b185a26b85e3fd6ebc40b1f445e6de840bae5440545c76ba66"
Nov 08 08:20:38 minikube kubelet[1689]: I1108 08:20:38.698418    1689 scope.go:117] "RemoveContainer" containerID="2075ce0ead2737c92be07c75a90f048fce84642330eb20743f56d2bc6dda1e39"
Nov 08 08:20:38 minikube kubelet[1689]: E1108 08:20:38.698858    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with CrashLoopBackOff: \"back-off 20s restarting failed container=nginx pod=nginx-dfddbd46c-4rdpl_front(2d757e66-ca2a-43e3-b429-4ae803d7ccf1)\"" pod="front/nginx-dfddbd46c-4rdpl" podUID="2d757e66-ca2a-43e3-b429-4ae803d7ccf1"
Nov 08 08:20:50 minikube kubelet[1689]: E1108 08:20:50.448603    1689 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:mysql,Image:mysql:9.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_USER,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-configmap,},Key:MYSQL_DATABASE,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_PASSWORD,Optional:nil,},},},EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-secret,},Key:MYSQL_ROOT_PASSWORD,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-pv-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rvlsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mysql-0_database(379b5da1-b164-4b83-b033-b26882502430): CreateContainerConfigError: configmap \"mysql-configmap\" not found" logger="UnhandledError"
Nov 08 08:20:50 minikube kubelet[1689]: E1108 08:20:50.450507    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"configmap \\\"mysql-configmap\\\" not found\"" pod="database/mysql-0" podUID="379b5da1-b164-4b83-b033-b26882502430"
Nov 08 08:20:51 minikube kubelet[1689]: I1108 08:20:51.440594    1689 scope.go:117] "RemoveContainer" containerID="2075ce0ead2737c92be07c75a90f048fce84642330eb20743f56d2bc6dda1e39"
Nov 08 08:20:51 minikube kubelet[1689]: E1108 08:20:51.440726    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with CrashLoopBackOff: \"back-off 20s restarting failed container=nginx pod=nginx-dfddbd46c-4rdpl_front(2d757e66-ca2a-43e3-b429-4ae803d7ccf1)\"" pod="front/nginx-dfddbd46c-4rdpl" podUID="2d757e66-ca2a-43e3-b429-4ae803d7ccf1"


==> storage-provisioner [7db23772e7ef] <==
I1108 07:37:19.051270       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1108 07:37:19.083968       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1108 07:37:19.085289       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1108 07:37:36.555883       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1108 07:37:36.556651       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_d5f54ddc-409b-46b6-966c-e8199035108f!
I1108 07:37:36.556023       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"418344e3-09a7-415a-b753-3b5bd60a040f", APIVersion:"v1", ResourceVersion:"13695", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_d5f54ddc-409b-46b6-966c-e8199035108f became leader
I1108 07:37:36.684182       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_d5f54ddc-409b-46b6-966c-e8199035108f!


==> storage-provisioner [8d5bc5f76cf1] <==
I1108 07:36:44.033470       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1108 07:37:05.113622       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

